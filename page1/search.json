[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jay Tilak Pentapati",
    "section": "",
    "text": "As an engineer from the Indian Institute of Petroleum and Energy, developed a strong foundation in engineering and project management. Passionate about exploring the intersection of sustainability and Machine Learning, with an aim to apply Machine Learning to address global environmental challenges.\nCurrently pursuing an M.Sc degree in Geographic Data Science at the University of Liverpool. Leveraging acquired skills to contribute to the development of innovative solutions for a sustainable future."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume/CV",
    "section": "",
    "text": "Note:\n\n\n\nPlease click button below CV to download it.\n\n\n\n\n\n\n\n\n\n\n\nOpen in new tab"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jay Tilak Pentapati",
    "section": "Education",
    "text": "Education\nUniversity of Liverpool | Liverpool, UK | September 2023 - Present\n       M.Sc in Geographic Data Science (Distinction, Expected Sep 2024)\n\n\nKey Modules: Geographic Data Science, Applied Geographic Information Systems, Spatial Modelling for Data Scientists, Web Mapping and Geo visualization.\n\nIndian Institute of Petroleum and Energy | India | August 2018 - May 2022\n       B.Tech in Petroleum Engineering (2:1)\n\n\nKey Modules: Advanced Drilling Engineering, Reservoir Engineering, Hydrocarbon Production Engineering, Engineering Mathematics, Engineering Mechanics, Geomechanics, Advanced Statistical Techniques."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jay Tilak Pentapati",
    "section": "Experience",
    "text": "Experience\n Royal Meteorological Society | Reading, UK | Oct 2023 - Present\n      Student Ambassador\n\n\nPromoting and raising awareness of the Royal Meteorological Society (RMetS) and the weather and climate sector among peers. Maintaining regular communication with RMetS, provide student insights for improving membership benefits.\n\n ONGC – Institute of Drilling Technology | India | Jul 2021 – Aug 2021\n      Summer Intern\n\n\nCollaborated with experts at ONGC to gain an understanding of the design process for well-control equipment, as well as compliance needed for all 121 Indian industry standards and safety regulations. Presented wellhead and Christmas tree design review findings to 7 peers and supervisors to ensure project alignment and progress tracking."
  },
  {
    "objectID": "CaseStudies.html",
    "href": "CaseStudies.html",
    "title": "Case Studies",
    "section": "",
    "text": "Analysing active satellites overhead for each country\n\n\n1 min\n\n\n\nPython\n\n\nspatial analysis\n\n\nGeopandas\n\n\nCelesTrak\n\n\nData Cleaning\n\n\nRESTFul API\n\n\n\nInteractive Map and RestFUL API\n\n\n\nJay T. Pentapati\n\n\nMar 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers\n\n\n33 min\n\n\n\nR\n\n\nspatial analysis\n\n\nAIRBNB\n\n\nData Cleaning\n\n\nRESTFul API\n\n\n\nInteractive Maps and Geodemographic Clustering with K-Means classfication method.\n\n\n\nJay T. Pentapati\n\n\nJan 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "add.html",
    "href": "add.html",
    "title": "My document",
    "section": "",
    "text": "Go Back\n\n\n\n\n\n\n\n\nNote:\n\n\n\nPlease click ‘Go Back’ Button for Central Page"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "My document",
    "section": "",
    "text": "Go Back\n\n\n\n\n\n\n\n\nNote:\n\n\n\nPlease click ‘Go Back’ Button for Central Page"
  },
  {
    "objectID": "AIRBNB_SanFran.html",
    "href": "AIRBNB_SanFran.html",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "",
    "text": "Go Back"
  },
  {
    "objectID": "satellites_overhead.html",
    "href": "satellites_overhead.html",
    "title": "Analysing active satellites overhead for each country",
    "section": "",
    "text": "Note:\n\n\n\nPlease click ‘Go Back’ Button for Central Page\n\n\n\n\nGo Back"
  },
  {
    "objectID": "AIRBNB_SanFran.html#installing-packages",
    "href": "AIRBNB_SanFran.html#installing-packages",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "1 Installing Packages",
    "text": "1 Installing Packages\nBelow code will check whether packages listed within variable (packages_required) are installed and not. If not installed, It will automatically installs and loaded all required packages for the Analysis.\n\n\nCode for Installing all required R packages.\npackages_required &lt;- c(\"tidycensus\", \"tigris\",\"tidyverse\", \"data.table\",\"sf\", \"tmap\", \"tmaptools\",\"readr\", \"geojsonsf\", \"osmdata\",\"devtools\", \"RColorBrewer\",\"classInt\",\"R.utils\",\"dplyr\", \"ggplot2\", \"viridis\",\"raster\",\"terra\",\"exactextractr\", \"tidyterra\", \"tibble\", \"rosm\", \"spdep\", \"cluster\", \"GGally\", \"rgeoda\", \"tidyr\", \"gridExtra\",\"patchwork\") \n# Include any additional packages needed, in above variable .\n\nnew_req_packages &lt;- packages_required[!(packages_required %in% installed.packages()[,\"Package\"])] # checking which packages are already installed and filtering out installed packages, new variable is assigned for names of uninstalled packages.\nif(length(new_req_packages)) install.packages(new_req_packages) # packages(not installed yet) are being installed. \n\nfor(i in 1:length(packages_required)) {\n  library(packages_required[i], character.only = T)        # loading all packages named in packages_required\n}\n# Below message is differently coded output for checking installed packages, not the output of above line\n\n\n\n[1] “All required packages for Assignment-2 are installed and loaded properly.”"
  },
  {
    "objectID": "AIRBNB_SanFran.html#introduction",
    "href": "AIRBNB_SanFran.html#introduction",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "2 Introduction",
    "text": "2 Introduction\n\n2.1 Rationale for CRS\nAccording to USGS Website, all maps in USA uses some projection in the State Plane Coordinate System (United States Geological Survey, 2023). It is plane coordinate system in which each state is divided up to six zones, depending on geometry of the state (United States Geological Survey, 2023). These zones use a Mercator or Lambert conic projection which preserves the shape and scale over smaller areas like zcta’s or tracts. \nBy looking into Map of USA State Plane Zones NAD83 in ArcGIS website (ArcGIS Hub, 2020), we can identify that Bay Area is in California Zone 3. Therefore, we are going to use EPSG: 2227 – NAD 83 California zone 3 (ftUS).  \n\n\n2.2 Description of Data\nAirbnb Listings Data of San Francisco, Oakland and San Mateo are extracted from Inside Airbnb Website. Chosen Variables from the list of socio-economic metrics of the ACS are Median Household Income (B19013_001E) and No. Of People aged 18-64 who has computer and Broadband Internet Subscription (B28005_011E). We also use Bay Area Zip Code Areas.\nFor additional Datasets, we are going to use No. of Vehicles (light-duty) 2020 per zip code and Average Electricity Utility Rate 2020 (Commerical Rate) per zip code, both investors owned and non-investor owned utilities. \nNote: API Key can be obtained by signing up at http://api.census.gov/data/key_signup.html for Census Data. We, then use tidycensus package to get ACS data from US Census Bureau.\n\n\nCode for setting up API Key\ncensus_api_key(J_API_KEY , install = TRUE, overwrite = TRUE) # Please replace 'J_API_KEY' with API Key obtained from census bureau\n# Below message is differently coded output for checking system environment variable(CENSUS_API_KEY), not the output of above line\n\n\n[1] “API key to get Census data is successfully set up.”\n\n\n2.3 Data Extraction and Pre-Processing\nWe are making two API calls to retrieve Median Household Income (B19013_001E) and the No. of people aged 18-64 who have a computer and Broadband Internet Subscription (B28005_011E) from ACS 2016-2020 separately. This approach is more efficient and straight-forward than making a single API call with two variables. The latter can results in a spatial or non-spatial file with a long-format attribute table, which, in turn, needs to be converted into a wide-format table for analysis.\nWe are extracting values of two variables for entire US. Then Later, we filter out zip codes specific to Bay Area. This is because, some califorina zcta’s extends into other states, Which results in error while extracting zcta’s related data for specific states (‘zip-Determining which US zipcodes map to more than one state or more than one city?-Geographic Information Systems’, 2021). According to US Census Bureau, All ZCTA’s codes are valid US Zip Codes(United States Census Bureau, 2023).\n\nSpatial Resolution: zcta’s level\nTime: 2020\n\n\n\nCleaning and pre-processing Data\nmedian_hh_income &lt;- get_acs(  # get_acs is a function, we are using to get ACS Suvrey data from US Census Bureau\n  geography = \"zcta\",         # we are specifying geography unit for variable we getting, like, ZCTA, Tract, County, State etc.\n  variables = \"B19013_001E\",  # we are geeting MEDIAN HOUSEHOLD INCOME, \"B19013_001E\" is variable name in ACS\n  year = 2020,      # we are getting census data collected from 2016-2020\n  geometry = FALSE  # we use geometry of bay area zipcode shapefile from berkeley library, not ACS Data\n) %&gt;%\n  rename(estimate_MedianHhI = estimate) %&gt;%   # we are renaming retrieved variable(estimate to appropriate name ( estimate_Median_HhI)\n  select(GEOID, estimate_MedianHhI)           # we are selecting only GEOID(ZCTA's CODE, also ZIP CODE), estimate Value (ignoring MOE)\n\n# Retrieving Internet Use Variable\ninternet_use &lt;- get_acs(      # get_acs is a function, we are using to get ACS Suvrey data from US Census Bureau \n  geography = \"zcta\",         # we are specifying geography unit for variable we getting, like, ZCTA, Tract, County, State etc.\n  variables = \"B28005_011E\",  # we are geeting No.of People aged 18-64, who has computer and Broadband Internet Subscription, \"B28005_011E\" is variable name in ACS\n  year = 2020,                # we are getting census data collected from 2016-2020\n  geometry = FALSE            #  we use geometry of bay area zipcode shapefile from berkeley library, not ACS Data\n) %&gt;%\n  rename(estimate_InternetUse = estimate) %&gt;%  # we are renaming retrieved variable(estimate to appropriate name ( estimate_Internet_Use)\n  select(GEOID, estimate_InternetUse)          # we are selecting only GEOID(ZCTA's CODE, also ZIP CODE), estimate Value (ignoring MOE)\n\nus_zip &lt;- left_join(median_hh_income, internet_use, by = \"GEOID\") # simply join two datasets by GEOID to get Dataset in req format  \n\n\n\n\nCode for Showing first six rows of Data being used\nhead(us_zip)  #showing first six rows of Dataset\n\n\n# A tibble: 6 × 3\n  GEOID estimate_MedianHhI estimate_InternetUse\n  &lt;chr&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1 00601              14398                 6839\n2 00602              16771                10955\n3 00603              15786                21193\n4 00606              14980                 1443\n5 00610              20167                 7742\n6 00612              19402                27116\n\n\nWe downloaded the Bay Area listings data (San Francisco, Oakland and San Mateo) from Airbnb Website and Bay Area Zip-Code Areas from Berkeley Library. Loading all .csv files by using read.csv function from utils package (Loading process can be found in source code of this file).\nTwo Additional Datasets:\nWe will be using Commerical Electricity Rates Dataset for Both investor-owned and non-investor owned utilities per ZIP Code.\n\n\nCode for Data cleaning and Pre-processing of Electricity rates data\nnon_iou_data_c &lt;- non_iou_data %&gt;%  # Data cleaning process for additional datasets, \n  select(zip, comm_rate) %&gt;%       # we select only zip, commerical rate from non-investor based utilities\n  group_by(zip) %&gt;%                # we group them by ZIP\n  summarise(\n    Avg_Non_IOU_Comm_Rate = mean(comm_rate, na.rm = TRUE)  # Mean of different Non-Investor Based utilities per ZIP\n  )\n  \n\niou_data_c &lt;- iou_data %&gt;%\n  select(zip, comm_rate) %&gt;%      # we select only zip, commerical rate from Investor based utilities\n  group_by(zip) %&gt;%               # we group them by ZIP\n  summarise(\n    Avg_IOU_Comm_Rate = mean(comm_rate, na.rm = TRUE)  # Mean of different Investor Based utilities per ZIP\n  )\n\nelectric_u &lt;- non_iou_data_c %&gt;%\n  full_join(iou_data_c, by = \"zip\")%&gt;%   # Join both Investor-owned and Non-Investor utility datasets by zipcode\n  mutate(\n    Avg_IOU_Comm_Rate = replace_na(Avg_IOU_Comm_Rate, 0),    # replacing any NA values with 0\n    Avg_Non_IOU_Comm_Rate = replace_na(Avg_Non_IOU_Comm_Rate, 0 ),  # replacing any NA values with 0\n    Combined_Avg_Comm_Rate = (Avg_Non_IOU_Comm_Rate + Avg_IOU_Comm_Rate ) / 2  # using average as statistic for combined Commerical electricity Rate\n  )%&gt;%\n  select(zip, Combined_Avg_Comm_Rate) %&gt;% rename(ZIP = zip) %&gt;% mutate(ZIP = as.character(ZIP)) # selecting only ZIP and New Statistic, ignoring rest\n\nvc_califorina &lt;- vehicle_count_2020 %&gt;%\n  select(-Date, -Model.Year, -Fuel, -Make) %&gt;%     # we are only selecting Zip.code, Vehicles and Duty cols, and ignoring rest\n  filter(Duty == \"Light\" & Zip.Code != \"OOS\") %&gt;%  # we are filter only Light-Duty vehicles and removing Out of Service(OOS) locations from data\n  group_by(Zip.Code) %&gt;%                           # group by ZIP Code\n  summarise(No_of_Light_Duty_Vehicles = sum(Vehicles)) %&gt;% rename( ZIP = Zip.Code)  # summing up Light-Duty Vehicles per ZIP and then rename it"
  },
  {
    "objectID": "AIRBNB_SanFran.html#methodology",
    "href": "AIRBNB_SanFran.html#methodology",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "3 Methodology",
    "text": "3 Methodology\n\n3.1 No. of AIRBNB Listings with Mean Price per ZIP Code Areas\nBay Area Airbnb’s Listings, We downloaded have Longitude and Longitude coordinates, So initially, We set CRS of Listings to EPSG: 4326 - WGS84. Then, We perform CRS re-projection into EPSG: 2227 – NAD 83 California zone 3 (ft-US).\nWe then, perform spatial join for Bay Area ZIP Code Areas and AIRBNB Listings, which results in Overlay Shape-file. This will enable us to understand geographical distribution of AIRBNB Listings and their charactertics over Bay Area ZIP Code Areas.\n\nCode for pre-processing of Shapefiles\nlistings_bayarea_sf &lt;- listings_bayarea %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\")) %&gt;% # creating point shapefile from coordinates\n  st_set_crs(4326)                                  # setting CRS to geodetic system of earth (4326, WGS84) as they are long, lat coords\n\nlistings_bayarea_sf_transform &lt;- st_transform(listings_bayarea_sf, crs = 2227)   # Now, Changing CRS to mentioned Project CRS\noverlay_listings &lt;- st_join(bayarea_zipcodes, listings_bayarea_sf_transform)     # Performing Spatial Join to create overlay Shapefile of Airbnb Listings and Bay area ZIP shapefile\n\nst_crs(overlay_listings) == st_crs(listings_bayarea_sf_transform)                # Verifying CRS of New shapefile\n\n[1] TRUE\nWe need to plot No. of Airbnb’s and their mean price per zipcode in Bay Area. To find out necessary variables, we group by Zip Code and count no. of non-NA price rows of listings in each zip code and use function(mean) to find mean price of those listings.\n\nlistings_bayarea_zip &lt;- overlay_listings  %&gt;%  # creating new dataframe with no.of Airbnb's per ZIP and Mean Price per ZIP\n  group_by(ZIP) %&gt;%                     # group by ZIP\n  summarise(\n    count_airbnb = sum(!is.na(price)),      # counting non-NA price rows for listings\n    mean_price = mean(price, na.rm = TRUE)  # calculate mean, ignoring NAs\n  )%&gt;%\n  filter(count_airbnb != 0) \n\n\nlistings_bayarea_zip1 &lt;- listings_bayarea_zip %&gt;%\n  select(count_airbnb, everything())  # Changing the order of attributes for interactive map\n\nlistings_bayarea_zip2 &lt;- listings_bayarea_zip %&gt;%\n  select(mean_price, everything())%&gt;% # Changing the order of attributes for interactive map\n  mutate(mean_price = round(mean_price, 1))  # rounding mean price to one decimal place.\n\nBased on standard statistics for both Count and Mean Price variables of AIRBNB’s per zip code, Minimum value of count variable is 1 and first quartile is 71, so first class is divided accordingly. Median of count variable is 172 and standard deviation is 162.5, so, second and third class is divided as defined. Finally to identify outliers, As Third quartile is 257, standard deviation is 162.5 and Maximum Value is 794, last class has 500 as its lower boundary.\nMinimum value of Mean Price variable is 71 and first quartile is 154, so first class is divided accordingly. Median of Mean Price is 204 and standard deviation is 297.9, so, second and third class is divided as defined. Finally to identify outliers, As Third quartile is 282.6, standard deviation is 297.9 and Maximum Value is 1818, last two classes have been divided 1000 and 1800 as their lower boundaries respectively.\nNew forms of spatial data like Airbnb data can offer latest snaphots of how people are are using spaces and leads to real-time insights. There will generally have high Heteroskedasticity and also provide detailed data at granular level such as individual Airbnbs. On other hand, New form of spatial data like Airbnb data never represent all segments of markets. Collection of Data can introduce measurement errors due to less standard methodologies.\nAirbnb data is dynamic and reflects real-time changes in market, like social media or GPS data. Like social Media or GPS data, Airbnb data is user-generated. This is quite different from census-type of data.\n\nCode for Maps of Count and Price of AIRBNBs per ZIP\ntmap_mode(\"view\")   # Interactive mode\nmap1.11 &lt;- tm_shape(listings_bayarea_zip1) +  # listings_bayarea_zip1 is just re-arrangement of listings_bayareaa_zip with first column as count_airbnb\n  tm_fill( \"count_airbnb\",style=\"fixed\", title = \"No. of Airbnb's per ZIP\", alpha = 1, breaks=c( 1, 72, 172, 359, 500, Inf), palette = \"-viridis\")+ \n  tm_borders()+\n  tm_text(\"ZIP\", size = 0.3)+\n  tm_view(set.view = c(center_lon, center_lat, zoom_level))\n\nmap1.12 &lt;- tm_shape(listings_bayarea_zip2) +  # listings_bayarea_zip2 is just re-arrangement of listings_bayareaa_zip with first column as Mean Price\n  tm_fill( \"mean_price\",style=\"fixed\", title = \"Mean Price of Airbnb per ZIP\", alpha = 1, breaks=c( 74, 155, 287, 501,1000, 1800, Inf), palette = \"-viridis\")+\n  tm_borders()+\n  tm_text(\"ZIP\", size = 0.3)+\n  tm_view(set.view = c(center_lon, center_lat, zoom_level))\n\ntmap_arrange(map1.11, map1.12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Median Household Income and No.of People aged 18-64 who has Computer and Internet Subscription per ZIP Code Areas\nWe filter Median Household Income and Internet Use Variable for Bay Area Zip Codes. Then we join ACS Dataset to Bay Area ZIP Code Areas shapefile. As an additional step, we remove all ZIP codes where Internet Use variable is missing.\n\n\nCode for Cleaning Attribute information in Shapefile\nbay_area_zipcodes &lt;- bayarea_zipcodes$ZIP   # Creating new var with Bayarea ZIP codes\n\nbayarea_zip_vars_data &lt;- us_zip %&gt;%        # Filtering ACS Data with Two variables, by Bay Area ZIP codes\n  filter(GEOID %in% bay_area_zipcodes)%&gt;%  # Using Bay Area ZIP codes variable, created above\n  rename(ZIP = GEOID)                     # renaming GEOID with ZIP\n\nbayarea_zip_vars &lt;- bayarea_zipcodes %&gt;% left_join(bayarea_zip_vars_data, by = \"ZIP\") %&gt;% filter(!(is.na(estimate_InternetUse))) # Joining two vars from ACS with bayarea_zipcodes shapefile which is downloaded from berkeley library and also removing NA values of Internet Use variable.\n\n\n\nhead(bayarea_zip_vars)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5936840 ymin: 2232344 xmax: 6251037 ymax: 2506800\nProjected CRS: NAD83 / California zone 3 (ftUS)\n# A tibble: 6 × 8\n  ZIP   PO_NAME   STATE       Area__ Length__                           geometry\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;MULTIPOLYGON [US_survey_foot]&gt;\n1 94558 NAPA      CA    12313263537   995176. (((6102909 2377436, 6102846 23769…\n2 95620 DIXON     CA     7236949521.  441860. (((6230747 2302747, 6219259 23030…\n3 95476 SONOMA    CA     3001414165.  311319. (((6013411 2248863, 6013202 22488…\n4 94559 NAPA      CA     1194301745.  359105. (((6045939 2248058, 6044555 22480…\n5 94533 FAIRFIELD CA      991786103.  200773. (((6146297 2299595, 6146371 22987…\n6 94954 PETALUMA  CA     2006544443.  267474. (((5998504 2235043, 5991182 22339…\n# ℹ 2 more variables: estimate_MedianHhI &lt;dbl&gt;, estimate_InternetUse &lt;dbl&gt;\n\n\nWe remove all ZIP codes where Median Household Income variable is missing.\n\n\nSummary of Median Household Income per Zipcode (estimate_MedianHhI): \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40813   89583  111037  122296  153256  250001 \n\n\n\nSummary of No.of People aged 18-64 who has Computer and Internet Subscription \n\n\nper Zipcode (estimate_InternetUse): \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0    8110   16235   17830   24914   59638 \n\n\nStandard Deviation of Variable(estimate_MedianHhI):     45357 \n\n\nStandard Deviation of Variable(estimate_InternetUse):   12765.53 \n\n\nBased on standard statistics for both variables of ACS per zip code, Range and Standard Deviation for both variables are large, We use Fisher-Jenks Data Classification method, because of highly-skewed data.\nBay Area ZIP Code Areas is Mixed-Use Type of Neighborhood. Most of neighborhoods have medium Household Incomes and High Internet Subscriptions. Socio-Economic Indicators like Median Household Income and Internet Use helps in delineating this typology of Neighborhood. San Francisco and Oakland have more-affulent neighborhoods due to High Internet Usage and medium Household Income, whereas San Mateo have High Household Income Neighborhoods and Medium Internet Usage.\nBased on the given Classification, Reasonable Hypothesis will be that AIRBNB would cluster in San Francisco and Oakland, due to High Internet Usage and Medium Household Income (Middle to High-Class Residential Areas). AIRBNB listings would also cluster in Areas with low commercial electricity rate and High No.of Vehicles.\n\n\n\nCode for Maps of Median Household Income and Internet Use per ZIP\ntmap_mode(\"plot\")\nmap2.1 &lt;- tm_basemap() +\n  tm_shape(bayarea_zip_vars1) +    # shapefile for map 2\n  tm_fill(col = \"estimate_MedianHhI\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Median Household Income (in $ USD)\") + # estimate_MedianHhI\n  tm_compass(position = c(0.85, 0.85)) +  # Adding Compass\n  tm_scale_bar(position = c(0.625, 0.02)) + # Adding Scalebar\n  tm_borders(lwd = 0.3)+      # Borders for zcta's polygons\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.22, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"Median Household Income per ZIP Code\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\nmap2.2 &lt;- tm_basemap() +\n  tm_shape(bayarea_zip_vars) +\n  tm_fill(col = \"estimate_InternetUse\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Broadband Internet Connection (People)\") +\n  tm_compass(position = c(0.85, 0.85)) +\n  tm_scale_bar(position = c(0.625, 0.02)) +\n  tm_borders(lwd = 0.3)+\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"No. of People aged (18-64) using Broadband Internet per ZIP Code\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\n\n\ntmap_arrange(map2.1, map2.2)\n\n\n\n\n\n\n\n\n\n\n3.3 MAP 3: AIRBNB Listings and Internet Use Variable (Combining Datasets)\nWe change the order of attributes in Bay Area ZIP Codes as following:\nWe calculate Natural Logarithm of Price of Airbnb Listings in Overlay Shapefile and Checking CRS of both shapefiles:\n\nCode for Calculating Log prices of AIRBNBs\nlistings_bayarea_sf_transform_copy &lt;- listings_bayarea_sf_transform\nlistings_bayarea_sf_transform_copy$nl_price &lt;- log(listings_bayarea_sf_transform_copy$price) # calculating natural logarithm of price ( log to base e)\nst_crs(bayarea_zip_vars_map3) == st_crs(listings_bayarea_sf_transform_copy) # checking CRS of both shapefiles\n\n[1] TRUE\nWe are changing the order of attribute in Overlay Listings Shapefile as following:\nBased on standard statistics for Internet Usage(People) per zip code and Natural Logarithm of AIRBNB Price, Range and Standard Deviation are large, We use Fisher-Jenks Data Classification method, because of highly-skewed data.\nAIRBNB Listings are majorly clustered in San Francisco, Daly City, South San Francisco, Burlingame, Foster City, Redwood City and Oakland areas(geographical observations from below map). High Natural Logarithm of Prices are found in San Francisco, West Coast Seaside and Redwood City(San Carlos).\nApparently in San Francisco, ZIP Code areas with High Internet Usage have High AIRBNB Listings. ZIP Code Areas with 5,500 people using internet, have high price for AIRBNB Listings.\n\nCode for Maps of Internet Use per ZIP in San Francisco\ntmap_mode(\"view\") # interactive mode activating\n\nmap3 &lt;- tm_shape(bayarea_zip_vars_map3) +\n  tm_fill(col = \"estimate_InternetUse\", alpha = 1, palette = \"YlGn\", n = 5, style = \"fisher\",title = \"Broadband Internet Connection(People)\") +\n  tm_borders() +\n  tm_text(\"ZIP\", size = 0.3)+\n  tm_shape(listings_bayarea_sf_transform_copy) +\n  tm_dots(col = \"nl_price\", palette = \"Reds\", n = 3, style = \"fisher\", title = \"Natural Logarithm of Airbnb's Price\") +\n  tm_view(set.view = c(center_lon, center_lat, zoom_level))# Set legend position for interactive view\n\nmap3\n\n\n\n\n\n3.4 MAP 4: Spatial Auto-Correlation\nCalculating list of neighbors(Neighborhoods) and Queen-based Spatial Weights Matrix for Bay Area ZIP Code areas which have Median Household Income:\n\n\nCode for calculating spatial weights martrix for ZIP codes which have Median Household Income\nnb_q1 &lt;- poly2nb(bayarea_zip_vars1, queen = TRUE) # shapefile used, have no NA values of Median Household Income variable\n\nw_queen1 &lt;- nb2listw(nb_q1, style = \"B\", zero.policy=TRUE) # queen-contiguity based Weights for list of neighbors for all observations \nisolates1 &lt;- which(w_queen1$neighbours == \"0\")\nbayarea_zip_vars1_NoZero &lt;- bayarea_zip_vars1[-c(isolates1),] # shapefile where there are no observation with zero neighbours based on queen-contiguity\n\n\nCalculating list of neighbors(Neighborhoods) and Queen-based Spatial Weights Matrix for Bay Area ZIP Code areas which have Internet Usage:\n\n\nCode for calculating spatial weights martrix for ZIP codes which have Internet Use\nnb_q2 &lt;- poly2nb(bayarea_zip_vars, queen = TRUE) # shapefile used, have no NA values of Broadband Internet Use\n\nw_queen2 &lt;- nb2listw(nb_q2, style = \"B\", zero.policy=TRUE)\nisolates2 &lt;- which(w_queen2$neighbours == \"0\")\n\nbayarea_zip_vars_NoZero &lt;- bayarea_zip_vars[-c(isolates2),] # shapefile where there are no observation with zero neighbours based on queen-contiguity\n\n\n\n3.4.1 Global Spatial Auto-Correlation:\nCalculating list of neighbors(Neighborhoods) and Queen-based Standardized Spatial Weights Matrix for Bay Area ZIP Code areas which have Median Household Income or/and Internet Usage(People) :\n\n\nCode for calculating spatial weights martrix for ZIP codes which have Median Household Income or Internet Use\nnb_q1 &lt;- poly2nb(bayarea_zip_vars1_NoZero, queen = TRUE) # Constructing neighbours list from filtering observations which have no zero neighbours and no NA values of Median Household Income\nw_queen_std1 &lt;- nb2listw(nb_q1, style = \"W\") # creating spatial weights matrix using queen contiguity and row-standardardised weights\n\nnb_q2 &lt;- poly2nb(bayarea_zip_vars_NoZero, queen = TRUE) # Constructing neighbours list from filtering out observations which have zero neighbours and no NA values of Broadband Internet Use\nw_queen_std2 &lt;- nb2listw(nb_q2, style = \"W\") # creating spatial weights matrix using queen contiguity and row-standardardised weights\n\n\nCalculating Spatial Lag for Median Household Income and Internet Usage(People):\n\n\nCode for calculating spatial lag for ZIP codes\nbayarea_zip_vars1_NoZero$sl_MedianHhI &lt;- lag.listw(w_queen_std1, bayarea_zip_vars1_NoZero$estimate_MedianHhI) # calculating spatial lag for Medain Household Income Variable in Bay Area \n\nbayarea_zip_vars_NoZero$sl_InternetUse &lt;-  lag.listw(w_queen_std2, bayarea_zip_vars_NoZero$estimate_InternetUse) # calculating spatial lag for Internet Use Variable in Bay Area \n\n\nMoran I’s for Median Household Income in Bay Area ZIP Code Areas:\n\n\nCode for calculating Moran’s I statitstic for Median Household Income\nmoran.mc(bayarea_zip_vars1_NoZero$estimate_MedianHhI, w_queen_std1, nsim=1000, alternative=\"greater\") # Moran's I statistic for Median Household Income Variable in Bay Area\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  bayarea_zip_vars1_NoZero$estimate_MedianHhI \nweights: w_queen_std1  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.39557, observed rank = 1001, p-value = 0.000999\nalternative hypothesis: greater\n\n\nMoran I’s for Median Household Income in Bay Area is 0.396 and p-value is 0.000999. So, Observed Spatial Pattern is statistically Significant. P-value means chances of obtaining Moran I’s value greater than original, given random spatial distribution. If it is less than 0.05, We reject Null Hypothesis i.e, observed spatial pattern is due to random chance. Moran I’s is analogous to Chi-Square Statistic. Spatial distribution of Median Household Income is spatially concentrated than random spatial distribution.\nMoran I’s for Internet Usage(People) in Bay Area ZIP Code Areas:\n\n\nCode for calculating Moran’s I statitstic for Internet Use\nmoran.mc(bayarea_zip_vars_NoZero$estimate_InternetUse, w_queen_std2, nsim=1000, alternative=\"greater\") # Moran's I statistic for Interent Use Variable in Bay Area\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  bayarea_zip_vars_NoZero$estimate_InternetUse \nweights: w_queen_std2  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.23313, observed rank = 1001, p-value = 0.000999\nalternative hypothesis: greater\n\n\nMoran I’s for Internet Usage (People) in Bay Area is 0.23313 and p-value is 0.000999. So, Observed Spatial Pattern is statistically Significant. Spatial distribution of Internet Usage (People) is mild spatially concentrated than random spatial distribution.\n\n\n3.4.2 Local Spatial Auto-Correlation:\nCalculating LISAs for Bay Area ZIP COde Areas for both Median Household Income and Internet Usage(People):\n\n\nCode for calculating LISA statistics\nlisa_perm1 &lt;- localmoran_perm(bayarea_zip_vars1_NoZero$estimate_MedianHhI, w_queen_std1, nsim=1000, alternative=\"two.sided\") # Lisa Clustering for Median Household Income in Bay Area\n\nlisa_perm2 &lt;- localmoran_perm(bayarea_zip_vars_NoZero$estimate_InternetUse, w_queen_std2, nsim=1000, alternative=\"two.sided\") # Lisa Clustering for Internet Use Variable in Bay Area\n\n\nCalculating Quadrants where local moran I’s for observation, with significance cutoff at 0.2. As there are no quadrants at significance level 0.1, it is set by seeing to have least no.of levels for at least one variable:\n\n\nCode for calculating LISA quadrants\nquadrants1 &lt;- hotspot(lisa_perm1, Prname=\"Pr(z != E(Ii)) Sim\", cutoff=0.2) # Creating quadrants from significance values of local Moran's I statistic calculated from Median Household Income\n\nquadrants2 &lt;- hotspot(lisa_perm2, Prname=\"Pr(z != E(Ii)) Sim\", cutoff=0.2) # Creating quadrants from significance values of local Moran's I statistic calculated from Internet Use Variable\n\n\n\n\nLevels of Quadrant 1 :\n\n\nLow-Low, High-Low, Low-High, High-High\n\n\n\n\nLevels of Quadrant 2 :\n\n\nLow-Low, High-Low, Low-High, High-High\n\n\nAdding Quadrants back to Bay Area ZIP Code Areas Shapefile:\n\n\nCode for adding new categorical variable back to shapefile\nbayarea_zip_vars1_NoZero$quadrant1 &lt;- as.character(quadrants1)  %&gt;% replace_na(\"Not significant\") # Adding created quadrants to shapefile(no NA values for Median Household Income) where no observation with zero neighbours based on queen-contiguity\n\n\nbayarea_zip_vars_NoZero$quadrant2 &lt;- as.character(quadrants2)  %&gt;% replace_na(\"Not significant\") # Adding created quadrants to shapefile(no NA values for Internet Use Variable) where no observation with zero neighbours based on queen-contiguity\n\n\n\n\n\n\n\n\nTo view complete chunks, see Source code.\nAs There were no quadrants when cutoff is at 0.1, We set cutoff at 0.2 to have minimum no. of Quadrants for atleast one variable. These LISA clusters are likely to arranged by random chance. It means Observed Clustering at whole Bay Area Level has Spatial Structure, whereas LISA Clusters observed are, may be due to random chance. Observed Spatial Pattern in LISA clusters is not statistically significant.\nBut, also, It is very highly unlikely to say that observed spatial pattern in LISA clusters is just by random chance. This requires further and more advanced analysis.\n\n\nCode for creating LISA maps\ntmap_mode(\"plot\")\nlisa_map_cluster1 &lt;- borders1 +\n  hh_map1 + ll_map1 + hl_map1  +lh_map1  + ns_map1 +   # combining all quadrant maps\n  tm_compass(position = c(0.85, 0.85)) +\n  tm_scale_bar(position = c(0.625, 0.02)) +\n  tm_add_legend(type = \"fill\", col = c(\"royalblue2\", \"red2\", \"darkgreen\", \"gold\", \"lightgrey\"), \n                labels = c(\"High-High\", \"Low-Low\", \"High-Low\", \"Low-High\", \"Not significant\"), title = \"LISA cluster(Median Household Income)\") +\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.22, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"LISA Clusters for Median Household Income Variable\", \n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\nlisa_map_cluster2 &lt;- borders2 +\n  hh_map2 + ll_map2 + hl_map2   + ns_map2 +  #  Combining All quadrant maps\n  tm_compass(position = c(0.85, 0.85)) +\n  tm_scale_bar(position = c(0.625, 0.02)) +\n  tm_add_legend(type = \"fill\", col = c(\"royalblue2\", \"red2\", \"darkgreen\", \"lightgrey\"), \n                labels = c(\"High-High\", \"Low-Low\", \"High-Low\", \"Not significant\"), title = \"LISA cluster(Broadband Internet Use)\") +\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"LISA Clusters for Broadband Internet Use Variable\",  \n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\n\ntmap_arrange(lisa_map_cluster1, lisa_map_cluster2)\n\n\n\n\n\n\n\n\n\n\n\n3.5 Geo-Demographic Classification in Bay Area\nWe create overlay shapefile by spatial join of Bay Area ZIP Code Areas and AIRBNB Listings. Then, we group by to calculate all variables. Also, Join Average Commercial Electricity rate and No.of Light-Duty Vehicles per Zip Code.\n\n\nCode for calculating dataframe required for Geodemographic Classification\noverlay_listings &lt;- st_join(bayarea_zip_vars, listings_bayarea_sf_transform,  join = st_intersects) # spatial join to create Overlay Shapefiles\n\noverlay_listings_zip &lt;- overlay_listings  %&gt;% \n  group_by(ZIP) %&gt;% \n  summarise(\n    count_airbnb =    sum(!is.na(price)),  # count non-NA price rows for count\n    mean_price =      mean(price, na.rm = TRUE), # Mean price \n    mean_review  =    mean(number_of_reviews, na.rm = TRUE), # No. of Reviews\n    mean_min_nights = mean(minimum_nights, na.rm = TRUE),   # No. of Minimum Nights stay\n    Median_HhI =      mean(estimate_MedianHhI, na.rm = TRUE), # Median Household income\n    Internet_Use =    mean(estimate_InternetUse, na.rm = TRUE) # Internet Usage\n  )%&gt;%\n  filter(count_airbnb != 0) \n\noverlay_listings_zip &lt;- overlay_listings_zip %&gt;% left_join(vc_califorina, by = \"ZIP\") # Join overlay listings with Light-Duty Vehicles count\noverlay_listings_zip &lt;- overlay_listings_zip %&gt;% left_join(electric_u, by = \"ZIP\") #Joining Overlay Listings with Commerical Electricity Rate.\n\noverlay_listings_zip &lt;- na.omit(overlay_listings_zip) # removing NA values\n\n\nWe create 8-tuple Vector for multi-variate classification:\n\n\nCode for sub-setting 8-tuple vector\nairbnb_ra &lt;- c('count_airbnb', 'mean_price', 'mean_review', 'mean_min_nights', 'Median_HhI', 'Internet_Use', 'No_of_Light_Duty_Vehicles', 'Combined_Avg_Comm_Rate') # geodemographic arguments - (8 - tuple) , multivariate classification\n\n\nSubsetting Overlay Listings Shapefile without geometry attribute:\n\n\nCode for sub-setting attribute information from shapefile\noverlay_listings_zip_NoGeo &lt;- st_drop_geometry(overlay_listings_zip[, airbnb_ra,  drop=FALSE]) # we are dropping geometry col in shp\n\noverlay_listings_zip_NoGeo &lt;- overlay_listings_zip_NoGeo[complete.cases(overlay_listings_zip_NoGeo), ] # removing NA Values\n\n\nK-Means Clustering:\n\nset.seed(12345)  # setting seed\n\nk6cls &lt;- kmeans(overlay_listings_zip_NoGeo, centers=6, iter.max = 1000) # KNN clusteing - 6 clusters (Each Observation will be 8-tuple )\noverlay_listings_zip$k6cls &lt;- k6cls$cluster   # Type of cluster assigned bacnk to overlay listings\n\n\nk6cls$centers # centres of 6 clusters, six 8-tuple vectors defines centre of each cluster\n\n  count_airbnb mean_price mean_review mean_min_nights Median_HhI Internet_Use\n1     279.0000   179.9462    53.98208        6.132616  106352.00     42383.00\n2     197.1875   278.3737    54.32059       15.048195  171773.62     17882.00\n3      48.0000   327.6437    35.09278       14.406388  242917.50      6848.00\n4     179.5385   438.6613    30.89614       15.389951   57687.23     15399.62\n5     202.0000   306.7908    59.36053       13.594888  128579.62     20916.00\n6     256.8235   174.3755    51.10254       14.275260   96413.88     26327.35\n  No_of_Light_Duty_Vehicles Combined_Avg_Comm_Rate\n1                 126455.00             0.09763171\n2                  19991.88             0.09186155\n3                  10437.00             0.13166995\n4                  14125.69             0.10183107\n5                  21415.62             0.09796353\n6                  25233.29             0.11473432\n\n\nBy observing centers of different clusters, Main types of Neighborhoods can be identified as Tourist-focused areas, Residential Areas, Economically-diverse areas and Vehicle-dependent areas etc. Tourist-Focused Areas can be identified by High Count, High Internet Usage, Low Median Household Income. Residential Areas can be identified by High Median Household Income, High Internet Usage, High No.of Light-duty Vehicles etc. Economically Diverse ares can be identified by mediummedian household income (Median_HhI) and internet usage (Internet_Use). Vehicle-dependent areas can be identified by High Vehile count.\nCharacteristics helping to delineate typology are AIRBNB Metrics(‘count_airbnb’, ‘mean_price’, ‘mean_review’, ‘mean_min_nights’), Median Household Income, Internet Usage(People), No. of Light-Duty vehicles, Average Electricity Commercial Rate.\nUsing this Classification, We can target areas most in need, by, like supporting Economically weak Neighborhoods which can be identified by Low Median Household Income, Internet Usage(People) for Development Programs and Infrastructure Improvements. For Managing Tourist Activity, Neighborhoods with high Airbnb count, urban planning committees can focus on minimizing impact of Tourism on local communities, like affordable housing. For Transportation and urban planning, Neighborhoods with High Vehicle count might benefit from improved Public Transportation and Development of Local Amenities to reduce travelling. For Community Services like Digital Hubs, Which might improve overall livelihood in Neighborhoods with Low Internet Usage and Varied Median Hosehold Income.\n\n\nCode for creating Geo-demographic clustering map\ntmap_mode(\"plot\")\n\nmap_cluster1 &lt;- tm_basemap() +\n  tm_shape(overlay_listings_zip) +\n  tm_fill(col = \"k6cls\", palette = viridis(256), style = \"cont\", title = \" Type of Cluster\") +\n  tm_compass(position = c(0.85, 0.88)) +\n  tm_scale_bar(position = c(0.58, 0.02)) +\n  tm_borders(col = \"white\", lwd = 0.2)+\n  tm_layout(\n    legend.title.size = 0.7,\n    legend.text.size = 0.6, \n    inner.margins = c(0.2, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"Type of Clusters in Bay Area\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\nmap_veh &lt;- tm_basemap() +\n  tm_shape(overlay_listings_zip) +\n  tm_fill(col = \"No_of_Light_Duty_Vehicles\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Vehicles per ZIP\") +\n  tm_compass(position = c(0.85, 0.88)) +\n  tm_scale_bar(position = c(0.58, 0.02)) +\n  tm_borders(col = \"black\", lwd = 0.2)+\n  tm_layout(\n    legend.title.size = 0.7,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"No. of Vehicles(Light-Duty) in Bay Area\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\nmap_electric &lt;- tm_basemap() +\n  tm_shape(overlay_listings_zip) +\n  tm_fill(col = \"Combined_Avg_Comm_Rate\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Commerical Electrical Rate\") +\n  tm_compass(position = c(0.85, 0.88)) +\n  tm_scale_bar(position = c(0.58, 0.02)) +\n  tm_borders(col = \"black\", lwd = 0.2)+\n  tm_layout(\n    legend.title.size = 0.7,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"Commercial Electrical Rate in Bay Area\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\n\ntmap_arrange(map_cluster1, map_veh, map_electric, nrow =1, ncol= 3)"
  },
  {
    "objectID": "AIRBNB_SanFran.html#conclusion",
    "href": "AIRBNB_SanFran.html#conclusion",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nGeo-Demographic Classification allows for making data-informed decisions for various policy making and Societal Improvements. In Reality, It is much more nuanced, we need to have understanding of different neighborhood dynamics, real-estate markets and other Socio-Economic Factors. But, With target areas based on neighborhood charactertistics, Policymakers can very effectively address local issues and enhance overall community well-being.\nThough Multi-Variate Clustering is performed, we neglected timeline factor, which introduces temporal inconsistencies. There can be biases introduced because of New forms of spatial data like AIRBNB Data. However, Apart from above insights in geodemographic classifcation, To make well-informed decisions for policymakers, We need more sophisticated approaches and Real-Time Verification procedures.\n\n\n\n\n\n\nNote:\n\n\n\nI apologize for any references I may have inadvertently forgotten to mention in this case study. All my work is built upon the invaluable contributions of the giants in the data science field, on whose shoulders I stand. I am deeply grateful for their insights and advancements, which have made this study possible."
  },
  {
    "objectID": "AIRBNB_SanFran.html#references",
    "href": "AIRBNB_SanFran.html#references",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "5 References",
    "text": "5 References\n\nGIS Geography. 2023. Choropleth Maps - A Guide to Data Classification. [online] Available at: https://gisgeography.com/choropleth-maps-data-classification/ (Accessed : 08 January 2024).\nRoss, S. (2009) Introduction to Probability and Statistics for Engineers and Scientists. 4th edn. London: Elsevier Academic Press.\nUnited States Geological Survey (2023) What is the State Plane Coordinate System? Can GPS provide coordinates in these values?. Available at: https://www.usgs.gov/faqs/what-state-plane-coordinate-system-can-gps-provide-coordinates-these-values#:~:text=The%20State%20Plane%20Coordinate%20System%20(SPCS)%2C%20which%20is%20only,the%20state%27s%20size%20and%20shape. (Accessed: 08 January 2024).\nUnited States Census Bureau (2023) ZIP Code Tabulation Areas (ZCTAs). Available at: https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html (Accessed: 08 January 2024).\nArcGIS Hub (2020) USA State Plane Zones NAD83. Available at: https://hub.arcgis.com/datasets/esri::usa-state-plane-zones-nad83/about (Accessed: 08 January 2024).\n‘zip-Determining which US zipcodes map to more than one state or more than one city?-Geographic Information Systems’ (2021) Geographic Information Systems Stack Exchange. Available at: https://gis.stackexchange.com/questions/53918/determining-which-us-zipcodes-map-to-more-than-one-state-or-more-than-one-city (Accessed: 08 January 2024).\nUC Berkeley GeoData Repository (2004) Bay Area ZIP Code Areas[Dataset]. Available at: https://geodata.lib.berkeley.edu/catalog/ark28722-s7888q (Accessed: 08 January 2024).\nCalifornia Open data (2023) Vehicle Fuel Type Count by Zip Code[Dataset]. Available at: https://data.ca.gov/dataset/vehicle-fuel-type-count-by-zip-code (Accessed: 08 January 2024).\nNational Renewable Energy Laboratory (2022) U.S. Electric Utility Companies and Rates: Look-up by Zipcode (2020)[Dataset]. Available at: https://catalog.data.gov/dataset/u-s-electric-utility-companies-and-rates-look-up-by-zipcode-2020 (Accessed: 08 January 2024)."
  },
  {
    "objectID": "AIRBNB_SanFran.html#methodology-1",
    "href": "AIRBNB_SanFran.html#methodology-1",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "4 Methodology",
    "text": "4 Methodology\n\n4.1 No. of AIRBNB Listings with Mean Price per ZIP Code Areas\nBay Area Airbnb’s Listings, We downloaded have Longitude and Longitude coordinates, So initially, We set CRS of Listings to EPSG: 4326 - WGS84. Then, We perform CRS re-projection into EPSG: 2227 – NAD 83 California zone 3 (ft-US).\nWe then, perform spatial join for Bay Area ZIP Code Areas and AIRBNB Listings, which results in Overlay Shape-file. This will enable us to understand geographical distribution of AIRBNB Listings and their charactertics over Bay Area ZIP Code Areas.\n\nCode for pre-processing of Shapefiles\nlistings_bayarea_sf &lt;- listings_bayarea %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\")) %&gt;% # creating point shapefile from coordinates\n  st_set_crs(4326)                                  # setting CRS to geodetic system of earth (4326, WGS84) as they are long, lat coords\n\nlistings_bayarea_sf_transform &lt;- st_transform(listings_bayarea_sf, crs = 2227)   # Now, Changing CRS to mentioned Project CRS\noverlay_listings &lt;- st_join(bayarea_zipcodes, listings_bayarea_sf_transform)     # Performing Spatial Join to create overlay Shapefile of Airbnb Listings and Bay area ZIP shapefile\n\nst_crs(overlay_listings) == st_crs(listings_bayarea_sf_transform)                # Verifying CRS of New shapefile\n\n[1] TRUE\nWe need to plot No. of Airbnb’s and their mean price per zipcode in Bay Area. To find out necessary variables, we group by Zip Code and count no. of non-NA price rows of listings in each zip code and use function(mean) to find mean price of those listings.\n\nlistings_bayarea_zip &lt;- overlay_listings  %&gt;%  # creating new dataframe with no.of Airbnb's per ZIP and Mean Price per ZIP\n  group_by(ZIP) %&gt;%                     # group by ZIP\n  summarise(\n    count_airbnb = sum(!is.na(price)),      # counting non-NA price rows for listings\n    mean_price = mean(price, na.rm = TRUE)  # calculate mean, ignoring NAs\n  )%&gt;%\n  filter(count_airbnb != 0) \n\n\nlistings_bayarea_zip1 &lt;- listings_bayarea_zip %&gt;%\n  select(count_airbnb, everything())  # Changing the order of attributes for interactive map\n\nlistings_bayarea_zip2 &lt;- listings_bayarea_zip %&gt;%\n  select(mean_price, everything())%&gt;% # Changing the order of attributes for interactive map\n  mutate(mean_price = round(mean_price, 1))  # rounding mean price to one decimal place.\n\nBased on standard statistics for both Count and Mean Price variables of AIRBNB’s per zip code, Minimum value of count variable is 1 and first quartile is 71, so first class is divided accordingly. Median of count variable is 172 and standard deviation is 162.5, so, second and third class is divided as defined. Finally to identify outliers, As Third quartile is 257, standard deviation is 162.5 and Maximum Value is 794, last class has 500 as its lower boundary.\nMinimum value of Mean Price variable is 71 and first quartile is 154, so first class is divided accordingly. Median of Mean Price is 204 and standard deviation is 297.9, so, second and third class is divided as defined. Finally to identify outliers, As Third quartile is 282.6, standard deviation is 297.9 and Maximum Value is 1818, last two classes have been divided 1000 and 1800 as their lower boundaries respectively.\nNew forms of spatial data like Airbnb data can offer latest snaphots of how people are are using spaces and leads to real-time insights. There will generally have high Heteroskedasticity and also provide detailed data at granular level such as individual Airbnbs. On other hand, New form of spatial data like Airbnb data never represent all segments of markets. Collection of Data can introduce measurement errors due to less standard methodologies.\nAirbnb data is dynamic and reflects real-time changes in market, like social media or GPS data. Like social Media or GPS data, Airbnb data is user-generated. This is quite different from census-type of data.\n\nCode for Maps of Count and Price of AIRBNBs per ZIP\ntmap_mode(\"view\")   # Interactive mode\nmap1.11 &lt;- tm_shape(listings_bayarea_zip1) +  # listings_bayarea_zip1 is just re-arrangement of listings_bayareaa_zip with first column as count_airbnb\n  tm_fill( \"count_airbnb\",style=\"fixed\", title = \"No. of Airbnb's per ZIP\", alpha = 1, breaks=c( 1, 72, 172, 359, 500, Inf), palette = \"-viridis\")+ \n  tm_borders()+\n  tm_text(\"ZIP\", size = 0.3)+\n  tm_view(set.view = c(center_lon, center_lat, zoom_level))\n\nmap1.12 &lt;- tm_shape(listings_bayarea_zip2) +  # listings_bayarea_zip2 is just re-arrangement of listings_bayareaa_zip with first column as Mean Price\n  tm_fill( \"mean_price\",style=\"fixed\", title = \"Mean Price of Airbnb per ZIP\", alpha = 1, breaks=c( 74, 155, 287, 501,1000, 1800, Inf), palette = \"-viridis\")+\n  tm_borders()+\n  tm_text(\"ZIP\", size = 0.3)+\n  tm_view(set.view = c(center_lon, center_lat, zoom_level))\n\ntmap_arrange(map1.11, map1.12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2 Median Household Income and No.of People aged 18-64 who has Computer and Internet Subscription per ZIP Code Areas\nWe filter Median Household Income and Internet Use Variable for Bay Area Zip Codes. Then we join ACS Dataset to Bay Area ZIP Code Areas shapefile. As an additional step, we remove all ZIP codes where Internet Use variable is missing.\n\n\nCode for Cleaning Attribute information in Shapefile\nbay_area_zipcodes &lt;- bayarea_zipcodes$ZIP   # Creating new var with Bayarea ZIP codes\n\nbayarea_zip_vars_data &lt;- us_zip %&gt;%        # Filtering ACS Data with Two variables, by Bay Area ZIP codes\n  filter(GEOID %in% bay_area_zipcodes)%&gt;%  # Using Bay Area ZIP codes variable, created above\n  rename(ZIP = GEOID)                     # renaming GEOID with ZIP\n\nbayarea_zip_vars &lt;- bayarea_zipcodes %&gt;% left_join(bayarea_zip_vars_data, by = \"ZIP\") %&gt;% filter(!(is.na(estimate_InternetUse))) # Joining two vars from ACS with bayarea_zipcodes shapefile which is downloaded from berkeley library and also removing NA values of Internet Use variable.\n\n\n\nhead(bayarea_zip_vars)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5936840 ymin: 2232344 xmax: 6251037 ymax: 2506800\nProjected CRS: NAD83 / California zone 3 (ftUS)\n# A tibble: 6 × 8\n  ZIP   PO_NAME   STATE       Area__ Length__                           geometry\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;MULTIPOLYGON [US_survey_foot]&gt;\n1 94558 NAPA      CA    12313263537   995176. (((6102909 2377436, 6102846 23769…\n2 95620 DIXON     CA     7236949521.  441860. (((6230747 2302747, 6219259 23030…\n3 95476 SONOMA    CA     3001414165.  311319. (((6013411 2248863, 6013202 22488…\n4 94559 NAPA      CA     1194301745.  359105. (((6045939 2248058, 6044555 22480…\n5 94533 FAIRFIELD CA      991786103.  200773. (((6146297 2299595, 6146371 22987…\n6 94954 PETALUMA  CA     2006544443.  267474. (((5998504 2235043, 5991182 22339…\n# ℹ 2 more variables: estimate_MedianHhI &lt;dbl&gt;, estimate_InternetUse &lt;dbl&gt;\n\n\nWe remove all ZIP codes where Median Household Income variable is missing.\n\n\nSummary of Median Household Income per Zipcode (estimate_MedianHhI): \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40813   89583  111037  122296  153256  250001 \n\n\n\nSummary of No.of People aged 18-64 who has Computer and Internet Subscription \n\n\nper Zipcode (estimate_InternetUse): \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0    8110   16235   17830   24914   59638 \n\n\nStandard Deviation of Variable(estimate_MedianHhI):     45357 \n\n\nStandard Deviation of Variable(estimate_InternetUse):   12765.53 \n\n\nBased on standard statistics for both variables of ACS per zip code, Range and Standard Deviation for both variables are large, We use Fisher-Jenks Data Classification method, because of highly-skewed data.\nBay Area ZIP Code Areas is Mixed-Use Type of Neighborhood. Most of neighborhoods have medium Household Incomes and High Internet Subscriptions. Socio-Economic Indicators like Median Household Income and Internet Use helps in delineating this typology of Neighborhood. San Francisco and Oakland have more-affulent neighborhoods due to High Internet Usage and medium Household Income, whereas San Mateo have High Household Income Neighborhoods and Medium Internet Usage.\nBased on the given Classification, Reasonable Hypothesis will be that AIRBNB would cluster in San Francisco and Oakland, due to High Internet Usage and Medium Household Income (Middle to High-Class Residential Areas). AIRBNB listings would also cluster in Areas with low commercial electricity rate and High No.of Vehicles.\n\n\n\nCode for Maps of Median Household Income and Internet Use per ZIP\ntmap_mode(\"plot\")\nmap2.1 &lt;- tm_basemap() +\n  tm_shape(bayarea_zip_vars1) +    # shapefile for map 2\n  tm_fill(col = \"estimate_MedianHhI\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Median Household Income (in $ USD)\") + # estimate_MedianHhI\n  tm_compass(position = c(0.85, 0.85)) +  # Adding Compass\n  tm_scale_bar(position = c(0.625, 0.02)) + # Adding Scalebar\n  tm_borders(lwd = 0.3)+      # Borders for zcta's polygons\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.22, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"Median Household Income per ZIP Code\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\nmap2.2 &lt;- tm_basemap() +\n  tm_shape(bayarea_zip_vars) +\n  tm_fill(col = \"estimate_InternetUse\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Broadband Internet Connection (People)\") +\n  tm_compass(position = c(0.85, 0.85)) +\n  tm_scale_bar(position = c(0.625, 0.02)) +\n  tm_borders(lwd = 0.3)+\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"No. of People aged (18-64) using Broadband Internet per ZIP Code\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\n\n\ntmap_arrange(map2.1, map2.2)\n\n\n\n\n\n\n\n\n\n\n4.3 MAP 3: AIRBNB Listings and Internet Use Variable (Combining Datasets)\nWe change the order of attributes in Bay Area ZIP Codes as following:\nWe calculate Natural Logarithm of Price of Airbnb Listings in Overlay Shapefile and Checking CRS of both shapefiles:\n\nCode for Calculating Log prices of AIRBNBs\nlistings_bayarea_sf_transform_copy &lt;- listings_bayarea_sf_transform\nlistings_bayarea_sf_transform_copy$nl_price &lt;- log(listings_bayarea_sf_transform_copy$price) # calculating natural logarithm of price ( log to base e)\nst_crs(bayarea_zip_vars_map3) == st_crs(listings_bayarea_sf_transform_copy) # checking CRS of both shapefiles\n\n[1] TRUE\nWe are changing the order of attribute in Overlay Listings Shapefile as following:\nBased on standard statistics for Internet Usage(People) per zip code and Natural Logarithm of AIRBNB Price, Range and Standard Deviation are large, We use Fisher-Jenks Data Classification method, because of highly-skewed data.\nAIRBNB Listings are majorly clustered in San Francisco, Daly City, South San Francisco, Burlingame, Foster City, Redwood City and Oakland areas(geographical observations from below map). High Natural Logarithm of Prices are found in San Francisco, West Coast Seaside and Redwood City(San Carlos).\nApparently in San Francisco, ZIP Code areas with High Internet Usage have High AIRBNB Listings. ZIP Code Areas with 5,500 people using internet, have high price for AIRBNB Listings.\n\nCode for Maps of Internet Use per ZIP in San Francisco\ntmap_mode(\"view\") # interactive mode activating\n\nmap3 &lt;- tm_shape(bayarea_zip_vars_map3) +\n  tm_fill(col = \"estimate_InternetUse\", alpha = 1, palette = \"YlGn\", n = 5, style = \"fisher\",title = \"Broadband Internet Connection(People)\") +\n  tm_borders() +\n  tm_text(\"ZIP\", size = 0.3)+\n  tm_shape(listings_bayarea_sf_transform_copy) +\n  tm_dots(col = \"nl_price\", palette = \"Reds\", n = 3, style = \"fisher\", title = \"Natural Logarithm of Airbnb's Price\") +\n  tm_view(set.view = c(center_lon, center_lat, zoom_level))# Set legend position for interactive view\n\nmap3\n\n\n\n\n\n4.4 MAP 4: Spatial Auto-Correlation\nCalculating list of neighbors(Neighborhoods) and Queen-based Spatial Weights Matrix for Bay Area ZIP Code areas which have Median Household Income:\n\n\nCode for calculating spatial weights martrix for ZIP codes which have Median Household Income\nnb_q1 &lt;- poly2nb(bayarea_zip_vars1, queen = TRUE) # shapefile used, have no NA values of Median Household Income variable\n\nw_queen1 &lt;- nb2listw(nb_q1, style = \"B\", zero.policy=TRUE) # queen-contiguity based Weights for list of neighbors for all observations \nisolates1 &lt;- which(w_queen1$neighbours == \"0\")\nbayarea_zip_vars1_NoZero &lt;- bayarea_zip_vars1[-c(isolates1),] # shapefile where there are no observation with zero neighbours based on queen-contiguity\n\n\nCalculating list of neighbors(Neighborhoods) and Queen-based Spatial Weights Matrix for Bay Area ZIP Code areas which have Internet Usage:\n\n\nCode for calculating spatial weights martrix for ZIP codes which have Internet Use\nnb_q2 &lt;- poly2nb(bayarea_zip_vars, queen = TRUE) # shapefile used, have no NA values of Broadband Internet Use\n\nw_queen2 &lt;- nb2listw(nb_q2, style = \"B\", zero.policy=TRUE)\nisolates2 &lt;- which(w_queen2$neighbours == \"0\")\n\nbayarea_zip_vars_NoZero &lt;- bayarea_zip_vars[-c(isolates2),] # shapefile where there are no observation with zero neighbours based on queen-contiguity\n\n\n\n4.4.1 Global Spatial Auto-Correlation:\nCalculating list of neighbors(Neighborhoods) and Queen-based Standardized Spatial Weights Matrix for Bay Area ZIP Code areas which have Median Household Income or/and Internet Usage(People) :\n\n\nCode for calculating spatial weights martrix for ZIP codes which have Median Household Income or Internet Use\nnb_q1 &lt;- poly2nb(bayarea_zip_vars1_NoZero, queen = TRUE) # Constructing neighbours list from filtering observations which have no zero neighbours and no NA values of Median Household Income\nw_queen_std1 &lt;- nb2listw(nb_q1, style = \"W\") # creating spatial weights matrix using queen contiguity and row-standardardised weights\n\nnb_q2 &lt;- poly2nb(bayarea_zip_vars_NoZero, queen = TRUE) # Constructing neighbours list from filtering out observations which have zero neighbours and no NA values of Broadband Internet Use\nw_queen_std2 &lt;- nb2listw(nb_q2, style = \"W\") # creating spatial weights matrix using queen contiguity and row-standardardised weights\n\n\nCalculating Spatial Lag for Median Household Income and Internet Usage(People):\n\n\nCode for calculating spatial lag for ZIP codes\nbayarea_zip_vars1_NoZero$sl_MedianHhI &lt;- lag.listw(w_queen_std1, bayarea_zip_vars1_NoZero$estimate_MedianHhI) # calculating spatial lag for Medain Household Income Variable in Bay Area \n\nbayarea_zip_vars_NoZero$sl_InternetUse &lt;-  lag.listw(w_queen_std2, bayarea_zip_vars_NoZero$estimate_InternetUse) # calculating spatial lag for Internet Use Variable in Bay Area \n\n\nMoran I’s for Median Household Income in Bay Area ZIP Code Areas:\n\n\nCode for calculating Moran’s I statitstic for Median Household Income\nmoran.mc(bayarea_zip_vars1_NoZero$estimate_MedianHhI, w_queen_std1, nsim=1000, alternative=\"greater\") # Moran's I statistic for Median Household Income Variable in Bay Area\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  bayarea_zip_vars1_NoZero$estimate_MedianHhI \nweights: w_queen_std1  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.39557, observed rank = 1001, p-value = 0.000999\nalternative hypothesis: greater\n\n\nMoran I’s for Median Household Income in Bay Area is 0.396 and p-value is 0.000999. So, Observed Spatial Pattern is statistically Significant. P-value means chances of obtaining Moran I’s value greater than original, given random spatial distribution. If it is less than 0.05, We reject Null Hypothesis i.e, observed spatial pattern is due to random chance. Moran I’s is analogous to Chi-Square Statistic. Spatial distribution of Median Household Income is spatially concentrated than random spatial distribution.\nMoran I’s for Internet Usage(People) in Bay Area ZIP Code Areas:\n\n\nCode for calculating Moran’s I statitstic for Internet Use\nmoran.mc(bayarea_zip_vars_NoZero$estimate_InternetUse, w_queen_std2, nsim=1000, alternative=\"greater\") # Moran's I statistic for Interent Use Variable in Bay Area\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  bayarea_zip_vars_NoZero$estimate_InternetUse \nweights: w_queen_std2  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.23313, observed rank = 1000, p-value = 0.000999\nalternative hypothesis: greater\n\n\nMoran I’s for Internet Usage (People) in Bay Area is 0.23313 and p-value is 0.000999. So, Observed Spatial Pattern is statistically Significant. Spatial distribution of Internet Usage (People) is mild spatially concentrated than random spatial distribution.\n\n\n4.4.2 Local Spatial Auto-Correlation:\nCalculating LISAs for Bay Area ZIP COde Areas for both Median Household Income and Internet Usage(People):\n\n\nCode for calculating LISA statistics\nlisa_perm1 &lt;- localmoran_perm(bayarea_zip_vars1_NoZero$estimate_MedianHhI, w_queen_std1, nsim=1000, alternative=\"two.sided\") # Lisa Clustering for Median Household Income in Bay Area\n\nlisa_perm2 &lt;- localmoran_perm(bayarea_zip_vars_NoZero$estimate_InternetUse, w_queen_std2, nsim=1000, alternative=\"two.sided\") # Lisa Clustering for Internet Use Variable in Bay Area\n\n\nCalculating Quadrants where local moran I’s for observation, with significance cutoff at 0.2. As there are no quadrants at significance level 0.1, it is set by seeing to have least no.of levels for at least one variable:\n\n\nCode for calculating LISA quadrants\nquadrants1 &lt;- hotspot(lisa_perm1, Prname=\"Pr(z != E(Ii)) Sim\", cutoff=0.2) # Creating quadrants from significance values of local Moran's I statistic calculated from Median Household Income\n\nquadrants2 &lt;- hotspot(lisa_perm2, Prname=\"Pr(z != E(Ii)) Sim\", cutoff=0.2) # Creating quadrants from significance values of local Moran's I statistic calculated from Internet Use Variable\n\n\n\n\nLevels of Quadrant 1 :\n\n\nLow-Low, High-Low, Low-High, High-High\n\n\n\n\nLevels of Quadrant 2 :\n\n\nLow-Low, High-Low, Low-High, High-High\n\n\nAdding Quadrants back to Bay Area ZIP Code Areas Shapefile:\n\n\nCode for adding new categorical variable back to shapefile\nbayarea_zip_vars1_NoZero$quadrant1 &lt;- as.character(quadrants1)  %&gt;% replace_na(\"Not significant\") # Adding created quadrants to shapefile(no NA values for Median Household Income) where no observation with zero neighbours based on queen-contiguity\n\n\nbayarea_zip_vars_NoZero$quadrant2 &lt;- as.character(quadrants2)  %&gt;% replace_na(\"Not significant\") # Adding created quadrants to shapefile(no NA values for Internet Use Variable) where no observation with zero neighbours based on queen-contiguity\n\n\n\n\n\n\n\n\nTo view complete chunks, see Source code.\nAs There were no quadrants when cutoff is at 0.1, We set cutoff at 0.2 to have minimum no. of Quadrants for atleast one variable. These LISA clusters are likely to arranged by random chance. It means Observed Clustering at whole Bay Area Level has Spatial Structure, whereas LISA Clusters observed are, may be due to random chance. Observed Spatial Pattern in LISA clusters is not statistically significant.\nBut, also, It is very highly unlikely to say that observed spatial pattern in LISA clusters is just by random chance. This requires further and more advanced analysis.\n\n\nCode for creating LISA maps\ntmap_mode(\"plot\")\nlisa_map_cluster1 &lt;- borders1 +\n  hh_map1 + ll_map1 + hl_map1  +lh_map1  + ns_map1 +   # combining all quadrant maps\n  tm_compass(position = c(0.85, 0.85)) +\n  tm_scale_bar(position = c(0.625, 0.02)) +\n  tm_add_legend(type = \"fill\", col = c(\"royalblue2\", \"red2\", \"darkgreen\", \"gold\", \"lightgrey\"), \n                labels = c(\"High-High\", \"Low-Low\", \"High-Low\", \"Low-High\", \"Not significant\"), title = \"LISA cluster(Median Household Income)\") +\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.22, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"LISA Clusters for Median Household Income Variable\", \n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\nlisa_map_cluster2 &lt;- borders2 +\n  hh_map2 + ll_map2 + hl_map2   + ns_map2 +  #  Combining All quadrant maps\n  tm_compass(position = c(0.85, 0.85)) +\n  tm_scale_bar(position = c(0.625, 0.02)) +\n  tm_add_legend(type = \"fill\", col = c(\"royalblue2\", \"red2\", \"darkgreen\", \"lightgrey\"), \n                labels = c(\"High-High\", \"Low-Low\", \"High-Low\", \"Not significant\"), title = \"LISA cluster(Broadband Internet Use)\") +\n  tm_layout(\n    legend.title.size = 0.715,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"LISA Clusters for Broadband Internet Use Variable\",  \n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\n\ntmap_arrange(lisa_map_cluster1, lisa_map_cluster2)\n\n\n\n\n\n\n\n\n\n\n\n4.5 Geo-Demographic Classification in Bay Area\nWe create overlay shapefile by spatial join of Bay Area ZIP Code Areas and AIRBNB Listings. Then, we group by to calculate all variables. Also, Join Average Commercial Electricity rate and No.of Light-Duty Vehicles per Zip Code.\n\n\nCode for calculating dataframe required for Geodemographic Classification\noverlay_listings &lt;- st_join(bayarea_zip_vars, listings_bayarea_sf_transform,  join = st_intersects) # spatial join to create Overlay Shapefiles\n\noverlay_listings_zip &lt;- overlay_listings  %&gt;% \n  group_by(ZIP) %&gt;% \n  summarise(\n    count_airbnb =    sum(!is.na(price)),  # count non-NA price rows for count\n    mean_price =      mean(price, na.rm = TRUE), # Mean price \n    mean_review  =    mean(number_of_reviews, na.rm = TRUE), # No. of Reviews\n    mean_min_nights = mean(minimum_nights, na.rm = TRUE),   # No. of Minimum Nights stay\n    Median_HhI =      mean(estimate_MedianHhI, na.rm = TRUE), # Median Household income\n    Internet_Use =    mean(estimate_InternetUse, na.rm = TRUE) # Internet Usage\n  )%&gt;%\n  filter(count_airbnb != 0) \n\noverlay_listings_zip &lt;- overlay_listings_zip %&gt;% left_join(vc_califorina, by = \"ZIP\") # Join overlay listings with Light-Duty Vehicles count\noverlay_listings_zip &lt;- overlay_listings_zip %&gt;% left_join(electric_u, by = \"ZIP\") #Joining Overlay Listings with Commerical Electricity Rate.\n\noverlay_listings_zip &lt;- na.omit(overlay_listings_zip) # removing NA values\n\n\nWe create 8-tuple Vector for multi-variate classification:\n\n\nCode for sub-setting 8-tuple vector\nairbnb_ra &lt;- c('count_airbnb', 'mean_price', 'mean_review', 'mean_min_nights', 'Median_HhI', 'Internet_Use', 'No_of_Light_Duty_Vehicles', 'Combined_Avg_Comm_Rate') # geodemographic arguments - (8 - tuple) , multivariate classification\n\n\nSubsetting Overlay Listings Shapefile without geometry attribute:\n\n\nCode for sub-setting attribute information from shapefile\noverlay_listings_zip_NoGeo &lt;- st_drop_geometry(overlay_listings_zip[, airbnb_ra,  drop=FALSE]) # we are dropping geometry col in shp\n\noverlay_listings_zip_NoGeo &lt;- overlay_listings_zip_NoGeo[complete.cases(overlay_listings_zip_NoGeo), ] # removing NA Values\n\n\nK-Means Clustering:\n\nset.seed(12345)  # setting seed\n\nk6cls &lt;- kmeans(overlay_listings_zip_NoGeo, centers=6, iter.max = 1000) # KNN clusteing - 6 clusters (Each Observation will be 8-tuple )\noverlay_listings_zip$k6cls &lt;- k6cls$cluster   # Type of cluster assigned bacnk to overlay listings\n\n\nk6cls$centers # centres of 6 clusters, six 8-tuple vectors defines centre of each cluster\n\n  count_airbnb mean_price mean_review mean_min_nights Median_HhI Internet_Use\n1     279.0000   179.9462    53.98208        6.132616  106352.00     42383.00\n2     197.1875   278.3737    54.32059       15.048195  171773.62     17882.00\n3      48.0000   327.6437    35.09278       14.406388  242917.50      6848.00\n4     179.5385   438.6613    30.89614       15.389951   57687.23     15399.62\n5     202.0000   306.7908    59.36053       13.594888  128579.62     20916.00\n6     256.8235   174.3755    51.10254       14.275260   96413.88     26327.35\n  No_of_Light_Duty_Vehicles Combined_Avg_Comm_Rate\n1                 126455.00             0.09763171\n2                  19991.88             0.09186155\n3                  10437.00             0.13166995\n4                  14125.69             0.10183107\n5                  21415.62             0.09796353\n6                  25233.29             0.11473432\n\n\nBy observing centers of different clusters, Main types of Neighborhoods can be identified as Tourist-focused areas, Residential Areas, Economically-diverse areas and Vehicle-dependent areas etc. Tourist-Focused Areas can be identified by High Count, High Internet Usage, Low Median Household Income. Residential Areas can be identified by High Median Household Income, High Internet Usage, High No.of Light-duty Vehicles etc. Economically Diverse ares can be identified by mediummedian household income (Median_HhI) and internet usage (Internet_Use). Vehicle-dependent areas can be identified by High Vehile count.\nCharacteristics helping to delineate typology are AIRBNB Metrics(‘count_airbnb’, ‘mean_price’, ‘mean_review’, ‘mean_min_nights’), Median Household Income, Internet Usage(People), No. of Light-Duty vehicles, Average Electricity Commercial Rate.\nUsing this Classification, We can target areas most in need, by, like supporting Economically weak Neighborhoods which can be identified by Low Median Household Income, Internet Usage(People) for Development Programs and Infrastructure Improvements. For Managing Tourist Activity, Neighborhoods with high Airbnb count, urban planning committees can focus on minimizing impact of Tourism on local communities, like affordable housing. For Transportation and urban planning, Neighborhoods with High Vehicle count might benefit from improved Public Transportation and Development of Local Amenities to reduce travelling. For Community Services like Digital Hubs, Which might improve overall livelihood in Neighborhoods with Low Internet Usage and Varied Median Hosehold Income.\n\n\nCode for creating Geo-demographic clustering map\ntmap_mode(\"plot\")\n\nmap_cluster1 &lt;- tm_basemap() +\n  tm_shape(overlay_listings_zip) +\n  tm_fill(col = \"k6cls\", palette = viridis(256), style = \"cont\", title = \" Type of Cluster\") +\n  tm_compass(position = c(0.85, 0.88)) +\n  tm_scale_bar(position = c(0.58, 0.02)) +\n  tm_borders(col = \"white\", lwd = 0.2)+\n  tm_layout(\n    legend.title.size = 0.7,\n    legend.text.size = 0.6, \n    inner.margins = c(0.2, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"Type of Clusters in Bay Area\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\nmap_veh &lt;- tm_basemap() +\n  tm_shape(overlay_listings_zip) +\n  tm_fill(col = \"No_of_Light_Duty_Vehicles\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Vehicles per ZIP\") +\n  tm_compass(position = c(0.85, 0.88)) +\n  tm_scale_bar(position = c(0.58, 0.02)) +\n  tm_borders(col = \"black\", lwd = 0.2)+\n  tm_layout(\n    legend.title.size = 0.7,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"No. of Vehicles(Light-Duty) in Bay Area\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\nmap_electric &lt;- tm_basemap() +\n  tm_shape(overlay_listings_zip) +\n  tm_fill(col = \"Combined_Avg_Comm_Rate\", palette = \"YlGn\", n = 5, style = \"fisher\", title = \"Commerical Electrical Rate\") +\n  tm_compass(position = c(0.85, 0.88)) +\n  tm_scale_bar(position = c(0.58, 0.02)) +\n  tm_borders(col = \"black\", lwd = 0.2)+\n  tm_layout(\n    legend.title.size = 0.7,\n    legend.text.size = 0.6, \n    inner.margins = c(0.185, 0.13, 0.02, 0.08), \n    legend.position = c(0.05, 0.02), \n    legend.width = 0.5, \n    bg.color = \"#eaf5fa\",\n    main.title = \"Commercial Electrical Rate in Bay Area\",  # Add the map title within tm_layout\n    main.title.size = 0.7,\n    main.title.position = \"center\"\n  )\n\n\n\ntmap_arrange(map_cluster1, map_veh, map_electric, nrow =1, ncol= 3)"
  },
  {
    "objectID": "satellites_overhead.html#installing-libaries",
    "href": "satellites_overhead.html#installing-libaries",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nInstalling Libaries\n",
    "text": "Installing Libaries\n\n\n\n# code for installing libaries\nimport subprocess\nimport sys\nimport importlib\n\n# List of required packages\nrequired_packages = [\"requests\", \"skyfield\",\"datetime\",\"numpy\", \"geopandas\", \"shapely\",\"json\",\"matplotlib\",\"pyproj\" , \"contextily\", \"mpl_toolkits\", \"mapclassify\", \"folium\"]  # Add all required packages here \n\n# defining Function to install packages\ndef install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n# Checking and installing missing packages\nfor package in required_packages:\n    try:\n        # Tries to import the package\n        importlib.import_module(package)\n    except ImportError:\n        # If the package is not found, this 'll install it\n        print(f\"{package} not found, installing...\")\n        install_package(package)\n\n# below message is differently coded output\n\n\n\n\n\nAll required packages are installed."
  },
  {
    "objectID": "satellites_overhead.html#introduction",
    "href": "satellites_overhead.html#introduction",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nIntroduction\n",
    "text": "Introduction\n\n\n\nAnalysis of Spatial Datasets through APIs\n\n\nIn the rapidly evolving field of Geoinforamtics to address complex scientific questions, the ability to access, analyze, and inference from spatial datasets stands as a cornerstone of research and development. Scientists around the world, trying to gain deeper understanding of many complex geographical phenomena, the dependence on accurate, latest and real-time spatial data has never been more critical. This Assignment uses richness of APIs (Application Programming Interfaces) to access different spatial datasets, offering opportunity to explore satellites/ objects patterns over geographic area.\n\n\nAPIs work as gatekeepers for programmatically retrieving data from remote servers, enabling researchers to tap into a vast amount of spatial data resources maintained by governments, non-profits, and private organizations. There are many types of APIs, but we are going to use RESTful API which is an type of API that relies on HTTP protocol to retrieve information from remote databases. Not all APIs which use HTTP protocol are RESTful.\n\n\nRESTful APIs means APIs which adheres to principles and constraints of REST, stands for Representational State Transfer, starting with Null style, which are client-server architecture, statelessness, cacheability, uniform interface, layered system and Code-On-Demand(optional) (Fielding, 2000). Terms ‘REST APIs’ and ‘RESTful APIs’ are often used interchangeably in context of web services, saying an APIs is RESTful typically implies strict compliance with REST principles or to REST Architectural Constraints. By leveraging API technology,\n\n\nFinally, This assignment aims to analyze active satellites overhead for each country, same methodology can be implemented to track space objects overhead. It also outlines methodology used in accessing and utilizing various spatial datasets for analysis through APIs.\n\n\n\n\nDatasets\n\n\nOpendatasoft’s Explore API was used to get geographical boundaries of all countries in GeoJSON format. It has limit of getting boundaries of 100 countries in one API Call. This dataset is called World Administrative Boundaries - Countries and Territories in Opendatasoft’s catalog (). Then, Active Satellites information is collected using API developed by Celes Trak (CelesTrak, 2024), This is also the data providers for TLE API which is one of the NASA APIs that provides two line element set data for earth orbiting objects in JSON format (NASA Open APIs, 2024).\n\n\n\n# Importing Required Packages with aliases and functions\nimport requests\nfrom skyfield.api import load, wgs84, EarthSatellite\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point, shape, Polygon, MultiPolygon\nimport json\nfrom shapely.ops import transform\nimport pyproj\nimport matplotlib.pyplot as plt\nimport contextily as ctx\n\nimport folium\nfrom mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\nfrom mapclassify import Quantiles\nimport matplotlib.font_manager as fm"
  },
  {
    "objectID": "satellites_overhead.html#tle-or-rarely-known-as-two-line-element-set",
    "href": "satellites_overhead.html#tle-or-rarely-known-as-two-line-element-set",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nTLE or rarely known as Two-Line Element Set\n",
    "text": "TLE or rarely known as Two-Line Element Set\n\n\nThree-Line Element Set (TLE) data is a type of format used to store orbital parameters of Earth-orbiting objects, commonly used by the satellite tracking and astronomy discipline. It is Developed by NORAD and NASA, a TLE consists of two lines of alphanumerical data that represent a satellite’s orbital parameters at a specific point in time, enabling the prediction of its future positions. Each line contains crucial information such as the satellite’s inclination, right ascension of the ascending node, eccentricity, argument of perigee, mean anomaly, mean motion, and the epoch of the element set (Space-Track.org, 2024). This format is instrumental for satellite tracking software to calculate satellite positions and velocities in the orbits of Earth. TLE data is widely used for various applications, including satellite communication, navigation, space debris tracking, and amateur satellite observation, making it a fundamental resource for understanding and monitoring man-made objects in Earth orbit.\n\n\nBelow we are going get TLE data for all active satellites orbiting the earth, using an API developed by Celes Trak(CelesTrak, 2024). Resulting response will be stored as plain text format. Then, we create an dataframe with satellite name, Line1 , Line 2 as columns.\n\n\n\n# Using Celes Trak, to get TLE Data for active satellites\ntry:\n    Sat_TLE = requests.get(\"https://celestrak.org/NORAD/elements/gp.php?GROUP=active&FORMAT=tle\")\n    if Sat_TLE.status_code == 200:\n     print(\"Data Retrieved successfully.\")\nexcept requests.exceptions.RequestException as e:\n    # This will catch HTTP errors, connection errors, etc\n    print(f\"An error occurred: {e}\")\n\n# first we split after every line, then we strip '\\r' at end of every line\ntle_lines = [line.strip('\\r') for line in Sat_TLE.text.strip().split('\\n')]\n\n# Checking if the number of lines is a multiple of 3\nif len(tle_lines) % 3 != 0:\n    print(\"Warning: The TLE data might be incomplete or corrupted.\")\n\nsatellites = [tle_lines[i:i+3] for i in range(0, len(tle_lines), 3)]\n\n\nData Retrieved successfully.\n\n\n\n\nsatellites_data = []\nfor sat in satellites:\n    # Each item in satellites list is a list with three elements: name, line1, line2\n    satellite_info = {\n        'name': sat[0],\n        'line1': sat[1],\n        'line2': sat[2]\n    }\n    satellites_data.append(satellite_info)\n\nsatellites_df = pd.DataFrame(satellites_data);\nsatellites_df.head()\n\n\n\n\n\n\n\n\n\nname\n\n\nline1\n\n\nline2\n\n\n\n\n\n\n0\n\n\nCALSPHERE 1\n\n\n1 00900U 64063C 24066.46276684 .00000999 0…\n\n\n2 00900 90.1969 53.1933 0024031 293.5009 184…\n\n\n\n\n1\n\n\nCALSPHERE 2\n\n\n1 00902U 64063E 24067.03319064 .00000061 0…\n\n\n2 00902 90.2105 56.7404 0020203 120.1107 309…\n\n\n\n\n2\n\n\nLCS 1\n\n\n1 01361U 65034C 24066.20400602 .00000011 0…\n\n\n2 01361 32.1460 354.8151 0004780 7.1482 352…\n\n\n\n\n3\n\n\nTEMPSAT 1\n\n\n1 01512U 65065E 24066.43969899 .00000056 0…\n\n\n2 01512 89.9487 214.5191 0071439 67.8035 357…\n\n\n\n\n4\n\n\nCALSPHERE 4A\n\n\n1 01520U 65065H 24066.59735979 .00000157 0…\n\n\n2 01520 89.9531 128.5739 0066944 292.8547 195…"
  },
  {
    "objectID": "satellites_overhead.html#satellite-coordinates-using-skyfield-library",
    "href": "satellites_overhead.html#satellite-coordinates-using-skyfield-library",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nSatellite Coordinates using Skyfield Library\n",
    "text": "Satellite Coordinates using Skyfield Library\n\n\nSkyfield library provides precise astronomical calculations for users, offering a interface to compute positions of planets, stars, and satellites within the solar system and beyond. Leveraging state-of-the-art algorithms and astronomical theories, Skyfield translates complex celestial mechanics into straightforward Python code, enabling the calculation of celestial events, satellite orbits (using TLE data), and astrophotography planning with high accuracy. This library is Ideal for our case, as we need to calculate satellites coordinates in given point in time, then it is possible to estimate satellites overhead per country at that instance.\n\n\n\n\nts = load.timescale() # timescale object from skyfield API, used to create time objects like UTC, delphi date\n\nstart_datetime = datetime(2024, 3, 4, 0, 0, 0)  # datetime object of 04 march 2024.\n# we want time intervals as column headers, so creating an times variable\ntimes = [(start_datetime + timedelta(minutes=240*i)).strftime('%Y-%m-%dT%H:%M:%SZ') for i in range(6)]\n\n# Defining Function to calculate satellite positions\ndef calculate_positions(name, line1, line2):\n    # Creating satellite object from TLE lines using Skyfield Library\n    satellite = EarthSatellite(line1, line2, name, ts)\n    \n    # saving an particular the start date\n    year, month, day = 2024, 3, 4\n    \n    # Generating a list of times at 240-minute intervals over 24 hours i.e. for every four hours\n    times = ts.utc(year, month, day, 0, range(0, 1440, 240))\n\n    # Getting  the satellite's position for every four hours using satellite object\n    geocentric = satellite.at(times)\n    subpoint = wgs84.subpoint(geocentric)\n    lons, lats = subpoint.longitude.degrees, subpoint.latitude.degrees\n    \n    \n    \n    # at end,  returns a list of formatted positions (longitude, latitude) to be consitent with shapley objects\n    return [f\"{lon},{lat}\" for lon, lat in zip(lons, lats)]\n\npositions_dict = {}  # defining empty dict to store satellite coords at diff time \n\n\n\nfor index, row in satellites_df.iterrows():\n    positions = calculate_positions(row['name'], row['line1'], row['line2'])\n    positions_dict[row['name']] = positions\n\n\n\n\n# Converting  above dictionary to a DataFrame\n\npositions_df = pd.DataFrame.from_dict(positions_dict, orient='index', columns=times) # times variable is used for column names\n\n# Resetting the index to move the satellite names into a regular column\npositions_df.reset_index(inplace=True)\n\n# Renaming the new column (previously the index) to 'Satellite'\npositions_df.rename(columns={'index': 'Satellite'}, inplace=True)\n\n\n\nTo have computational efficiency, We used dictionary type object in intermediate process of function (calculate_positions). Latitude and Longitude are stored as string dataframe(postions_df). We will convert every coords which are as strings to Point Geometry using Shapely Library.\n\n\nNote: In python, we use (longtiude, latitude). Below code splits the string for each value when encountered an comma and converts to float-type data before passing into Point Function in Shapely Library.\n\n\n\n\nfor timestamp in times:\n    positions_df[timestamp] = positions_df[timestamp].apply(lambda x: Point([float(coord) for coord in x.split(',')]))\n\n\n\n\npositions_df.head()\n\n\n\n\n\n\n\n\n\nSatellite\n\n\n2024-03-04T00:00:00Z\n\n\n2024-03-04T04:00:00Z\n\n\n2024-03-04T08:00:00Z\n\n\n2024-03-04T12:00:00Z\n\n\n2024-03-04T16:00:00Z\n\n\n2024-03-04T20:00:00Z\n\n\n\n\n\n\n0\n\n\nCALSPHERE 1\n\n\nPOINT (70.90660296193354 3.7158649204721845)\n\n\nPOINT (-168.2230109900412 -79.3722281604938)\n\n\nPOINT (130.47883777703515 25.49233304337693)\n\n\nPOINT (-109.35040111178193 50.56658383501217)\n\n\nPOINT (-170.0184818555208 -53.94570821806006)\n\n\nPOINT (-49.83228886040186 -21.69938786339812)\n\n\n\n\n1\n\n\nCALSPHERE 2\n\n\nPOINT (-106.04522979299136 66.0981297857472)\n\n\nPOINT (14.353289669368113 22.48952566357138)\n\n\nPOINT (-46.43833895820065 -68.951960658235)\n\n\nPOINT (74.02369011926766 -20.55807177289187)\n\n\nPOINT (13.18149034330885 70.84603085910078)\n\n\nPOINT (133.69052095595006 17.695467848233477)\n\n\n\n\n2\n\n\nLCS 1\n\n\nPOINT (-103.73793219985143 28.275895642548118)\n\n\nPOINT (77.86859020381276 -28.504394266699823)\n\n\nPOINT (-111.00718163893485 4.824675764611408)\n\n\nPOINT (56.45280892389135 22.21917838120799)\n\n\nPOINT (-123.61622597210805 -31.793924004348426)\n\n\nPOINT (52.124324310310705 13.783768480659568)\n\n\n\n\n3\n\n\nTEMPSAT 1\n\n\nPOINT (-127.6281109549757 -60.15136124263363)\n\n\nPOINT (-7.927512238827062 -41.61642564191508)\n\n\nPOINT (-68.00651525619188 38.61770438385358)\n\n\nPOINT (51.6960768856886 60.85531663368477)\n\n\nPOINT (-8.359777281330288 -19.156520843701077)\n\n\nPOINT (111.07009588726672 -82.49206121312446)\n\n\n\n\n4\n\n\nCALSPHERE 4A\n\n\nPOINT (146.4287344452664 -62.92172296394485)\n\n\nPOINT (-93.86073668305397 -35.400617125788244)\n\n\nPOINT (-153.9434423479504 46.50453062900567)\n\n\nPOINT (-34.22041900967933 53.366512005072416)\n\n\nPOINT (-94.29846233060641 -27.457520890430107)\n\n\nPOINT (25.375801967620593 -71.12113770784784)\n\n\n\n\n\n\n\n\n\nRationale for CRS\n\n\nAs this assignment covered, satellites overhead per country, which involves calculating coordinates of satellites i.e. Latitude and Longitude. We are going to use EPSG:4326-WGS84 as CRS for our spatial analyses, also for its compatibility with global positioning systems (GPS) and web mapping services. WGS84 is cordinate reference system for GPS satellites (GISGeography, 2023)."
  },
  {
    "objectID": "satellites_overhead.html#world-administrative-boundaries",
    "href": "satellites_overhead.html#world-administrative-boundaries",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nWorld Administrative Boundaries\n",
    "text": "World Administrative Boundaries\n\n\nOpenDataSoft is a comprehensive website designed to ease the process of publishing, sharing, and utilization of data by organizations and governments. It facilitates users to have access to a wide range of datasets across various domains, enhancing innovation, and collaboration. This API provided by OpenDataSoft, offers access to detailed geographic boundaries of countries and administrative regions around the globe ().\n\n\n\n\n\nbase_url = 'https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/world-administrative-boundaries/records'\ntotal_count = 256     # Total no.of countries exists, according to opendatasoft server\nlimit = 100  # Number of results per API call, According to opendatasoft site\noffset = 0  # starting at the beginning\n\nall_countries = []  # empty list for adding info \n\nwhile offset &lt; total_count:\n    response = requests.get(base_url, params={'limit': limit, 'offset': offset})\n    if response.status_code == 200:\n        data = response.json()   # converting into JSON\n        all_countries.extend(data['results'])  # Extracting 'results' item in list\n        offset += limit  # For API call to get next set of results\n        # We dont need to trim all_countries because API has correctly 256 countries, no additional polygons\n    else:\n        print(f\"Failed to fetch data: HTTP {response.status_code}\")\n        break  # will exit the loop in case of an error\n\n# Now, `all_countries` contains data for up to 256 countries, fetched in chunks of 100\n\n\n\nThe GeoJSON retrieved from the OpenDataSoft API for world administrative boundaries deviates from the standard naming in GeoJSON structure, which needed an additional cleaning process to ensure compatibility. To address these issues, like features are stored as results in GeoJSON, So I modified response by reformatting the GeoJSON to align with the standards as below.\n\n\n\n\n# Assuming `data` is your JSON object\nresults = all_countries\n\n# Preparing lists to hold data from 'results'\ngeometries = []\nproperties = []\n\nfor item in results:\n    if 'geo_shape' in item and 'geometry' in item['geo_shape'] and 'name' in item:\n        geom = shape(item['geo_shape']['geometry'])\n        geometries.append(geom)\n        properties.append(item['name'])\n        \n    else:\n        geometries.append(None)\n        properties.append(None)\n\n        \n\ncountries_df = pd.DataFrame({'Countries' : properties})\n\n# Converting the DataFrame to a GeoDataFrame\ncountries_gdf = gpd.GeoDataFrame(countries_df, geometry=geometries);\n\ncountries_gdf.set_crs(epsg=4326, inplace=True) # setting crs to EPSG: 4326\n\ncountries_gdf.head()\n\n\n\n\n\n\n\n\n\nCountries\n\n\ngeometry\n\n\n\n\n\n\n0\n\n\nTurks and Caicos Islands\n\n\nMULTIPOLYGON (((-71.12757 21.44263, -71.14605 …\n\n\n\n\n1\n\n\nBolivia\n\n\nPOLYGON ((-58.15889 -20.16806, -58.13722 -20.1…\n\n\n\n\n2\n\n\nCuba\n\n\nMULTIPOLYGON (((-82.54460 21.57389, -82.59862 …\n\n\n\n\n3\n\n\nChina\n\n\nMULTIPOLYGON (((110.71583 20.06888, 110.77859 …\n\n\n\n\n4\n\n\nIraq\n\n\nPOLYGON ((44.78734 37.14971, 44.76617 37.11228…\n\n\n\n\n\n\n\n\nAs we can see from the ‘geometry’ column, some of them are polygons, we need to ensure consistency for mapping and visualizations So, we convert to polygons into MultiPolygon. Conversion function, defined, is hided in cell.\n\n\n\n# Applying the conversion function (hided in above cell ) to the 'geometry' column of your GeoDataFrame\ncountries_gdf['geometry'] = countries_gdf['geometry'].apply(ensure_multipolygon);\n\n\n\n\nMap Projection Problem\n\n\nThe issue of map projections where geography of countries extending into the International Date Line presents a challenge in plotting latitude and longitude on 2D maps, particularly for areas like east russia near the 180-degree meridian. This problem arises due to the representation of the Earth’s surface on a two-dimensional plane which requires distortions, and the International Date Line acts as a discontinuity for spatial datasets. As a result, geographical features that span across this meridian may appear fragmented or inaccurately placed on either side of the map. Below code rectifies this issue and adds geometry back to Geo dataframe(countries_gdf).\n\n\n\n# Before Shift function, and it is hided in above chunk \nrussia_gdf_bs= countries_gdf[countries_gdf['Countries'] == 'Russian Federation']\n\n# Applying the shift function to the Russian geometries.\nrussia_gdf = countries_gdf[countries_gdf['Countries'] == 'Russian Federation'].copy()\nrussia_gdf['geometry'] = russia_gdf['geometry'].apply(lambda geom: shift_geom(geom, 180))\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 14))  # 1 row, 2 columns\n\n# first GeoDataFrame\nrussia_gdf_bs.plot(ax=axs[0], color='blue')  # Plot on the first subplot\naxs[0].set_title('Geography of Russia on 2D Map - Before using Shift Function') \n\n# Increasing the y-limit of the left plot\nymin, ymax = axs[0].get_ylim()\naxs[0].set_ylim(ymin*0.6, ymax * 1.4)\n\n#  second GeoDataFrame\nrussia_gdf.plot(ax=axs[1], color='green')  # Plot on the second subplot\naxs[1].set_title('Geography of Russia on 2D Map - After using Shift Function')  \n\n\nplt.tight_layout() \nplt.show()"
  },
  {
    "objectID": "satellites_overhead.html#estimation-of-satellites-overhead",
    "href": "satellites_overhead.html#estimation-of-satellites-overhead",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nEstimation of Satellites Overhead\n",
    "text": "Estimation of Satellites Overhead\n\n\nIn this assignment, I am interested to analyze the spatial distribution of satellite positions over time and their intersection with various countries around the globe. The core of below process involves iterating over a series of timestamps, each representing a distinct snapshot of satellite positions. For each timestamp, we extracted corresponding satellite position data and transformed it into a GeoDataFrame. This transformation allowed to perform a spatial join with a pre-existing GeoDataFrame of country boundaries, effectively determining which satellites were located within the boundaries of each country at any given moment.\n\n\nThen, we aggregated these results to compute the count of satellites per country for each timestamp, generating a Series object that mapped each country to its respective satellite count. These counts were then combined into a new DataFrame, where columns represented individual timestamps and rows corresponded to different countries. This DataFrame can serve as a temporal satellite count matrix, capturing the dynamics of satellite distributions across countries over time.\n\n\nTo facilitate visualization of static and interactive maps, we merged this matrix with our original GeoDataFrame of country boundaries. This merge operation creates a unified dataset that has spatial resolution of countries and temporal resolution of 4 hours. Due to constraints of assignment, We calculated satellites distribution with temporal resolution of 4 hours. This resultant spatial dataset can be used to find insights into various issues regarding global digital repression, mass surveillance etc.\n\n\n\n# temporal satellite count matrix estimtion\n\n# empty dict var\ncounts_dict = {}\n\nfor timestamp in positions_df.columns[1:]:  # Skipping the 'name' column\n   \n    # creating copy of first(Satellite), second col of postions_df\n\n    temp_df = positions_df[['Satellite', timestamp]].copy()\n    temp_df.rename(columns={timestamp: 'position'}, inplace=True)\n    \n    \n    # Convert the DataFrame to a GeoDataFrame\n    temp_gdf = gpd.GeoDataFrame(temp_df, geometry='position')\n    temp_gdf.set_crs(epsg=4326, inplace=True)  # Assuming positions are in lat/lon\n    \n    countries_gdf_copy = countries_gdf_copy.dropna(subset=['Countries'])\n    # Performing spatial join with countries_gdf to find positions within each country\n    joined_gdf = gpd.sjoin(temp_gdf, countries_gdf_copy, how=\"inner\", predicate='intersects')\n\n    counts_series = joined_gdf.groupby('Countries').size()  # Placeholder for your count computation logic\n    counts_series = counts_series.reindex(countries_gdf_copy['Countries'].unique(), fill_value=0)\n    \n    counts_dict[timestamp] = counts_series\n\n# Converting the dictionary of Series to a DataFrame\ncounts_df = pd.DataFrame(counts_dict)\n\n# Ensuring the index of counts_df matches countries_gdf (e.g., country names)\n\n# Reset the index of counts_df\ncounts_df_reset = counts_df.reset_index()\ncounts_df_reset.rename(columns={'index': 'Countries'}, inplace=True)\n\n# Merge counts_df_reset with countries_gdf\ntemporal_satellite_cm = pd.merge(countries_gdf_copy, counts_df_reset, how='left', on='Countries')\n\ntemporal_satellite_cm.head()\n\n\n\n\n\n\n\n\n\nCountries\n\n\ngeometry\n\n\n2024-03-04T00:00:00Z\n\n\n2024-03-04T04:00:00Z\n\n\n2024-03-04T08:00:00Z\n\n\n2024-03-04T12:00:00Z\n\n\n2024-03-04T16:00:00Z\n\n\n2024-03-04T20:00:00Z\n\n\n\n\n\n\n0\n\n\nTurks and Caicos Islands\n\n\nMULTIPOLYGON (((-71.12757 21.44263, -71.14605 …\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\nBolivia\n\n\nMULTIPOLYGON (((-58.15889 -20.16806, -58.13722…\n\n\n20\n\n\n19\n\n\n10\n\n\n12\n\n\n24\n\n\n15\n\n\n\n\n2\n\n\nCuba\n\n\nMULTIPOLYGON (((-82.54460 21.57389, -82.59862 …\n\n\n3\n\n\n1\n\n\n0\n\n\n3\n\n\n5\n\n\n2\n\n\n\n\n3\n\n\nChina\n\n\nMULTIPOLYGON (((110.71583 20.06888, 110.77859 …\n\n\n190\n\n\n222\n\n\n195\n\n\n180\n\n\n214\n\n\n202\n\n\n\n\n4\n\n\nIraq\n\n\nMULTIPOLYGON (((44.78734 37.14971, 44.76617 37…\n\n\n9\n\n\n9\n\n\n5\n\n\n9\n\n\n6\n\n\n8\n\n\n\n\n\n\n\n\n\nData Classifications\n\n\n\n# plot_classification function hided above\nplot_classifications(temporal_satellite_cm, '2024-03-04T00:00:00Z')\n\n\n\n\n\n\n\n\n\n\n\n\nAfter observing distribution of satellites distribution at 2024-03-04T00:00:00Z , To distinguish High satellites overhead from low, We can plot either Natural Breaks or fisher jenks classification. Therefore, We are going to plot Natural Breaks classification map along with normal map.\n\n\n\n\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 15))  # 1 row, 2 columns for side by side plots\n\n# Plot 1: Simple color map\ntemporal_satellite_cm.plot(column='2024-03-04T00:00:00Z', ax=axs[0], legend=True,\n                           cmap='Greens', edgecolor='black', lw=0.6)\naxs[0].set_title(f\"No. of Satellites Overhead per country - {times[0]}\")\n\n\n# Increasing the y-limit of the left plot\nymin, ymax = axs[0].get_ylim()\naxs[0].set_ylim(ymin, ymax * 1.3)\n# Increasing the x-limit of the left plot\nxmin, xmax = axs[0].get_xlim()\naxs[0].set_xlim(xmin*0.975, xmax * 1)\nadd_scalebar(axs[0], length=5556, location=(0.01, 0.05), units='km')\n\n\n# Plot 2: Natural breaks classification\ntemporal_satellite_cm.plot(column='2024-03-04T00:00:00Z', ax=axs[1], legend=True,\n                           cmap='YlGnBu', scheme='naturalbreaks', k=5, \n                           edgecolor='black', lw=0.6, legend_kwds={'loc': 'center right'})\naxs[1].set_title(f\"No. of Satellites Overhead per country (Natural Breaks) - {times[0]}\")\nadd_scalebar(axs[1], length=5556, location=(0.01, 0.05), units='km')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistance calculated from equator (0 N, 0 E) to (0 N, 50 E) - https://www.nhc.noaa.gov/gccalc.shtml"
  },
  {
    "objectID": "satellites_overhead.html#interactive-map",
    "href": "satellites_overhead.html#interactive-map",
    "title": "Exploring Various Datasets of Bay Area and Finding Useful Insights for Policymakers",
    "section": "\nInteractive Map\n",
    "text": "Interactive Map\n\n\n\n# below map use mapbbox basemap  mbox_url = f'https://api.mapbox.com/styles/v1/mapbox/light-v9/tiles/"
  }
]